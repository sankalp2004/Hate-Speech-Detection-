{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankalp2004/Hate-Speech-Detection-/blob/main/HateSpeechShown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "94b9d857840c4e289714915796d694a2",
            "7e3667414fc84857a58a1162dffc007d",
            "9bc037757ecb4b199fe3c604e405b0c5",
            "00dd1d2affc54aa5bb652f623570685e",
            "9616c890ef9540eeb8088688ae90c6d1",
            "72f468e32d3c4d82862347e96f29a884",
            "0c146814ac234918ad18dd04da917cdc",
            "83ebfb9b766b4662aaa5b61ad0b7f53e",
            "e2b3f307ffba44358094fd046d91543e",
            "6bc23a3737b143ff9f64cb811c06f281",
            "c9004cdc57ee4f5fb6936da1aafe5dd8",
            "d1994c77e9394fc1b550871c7501f4c4",
            "62f19090e0784a3aa9fcb82507693663",
            "deb2db83c6974209a769fe26802f4eda",
            "29e246ff10b0423bb10f2221edfa80b9",
            "48d1af1c60c44b83bf328f3e857db869",
            "54cb39be9d3a44999b606501f6ac8aba",
            "b82068c5cc0e450a8129aeebd0847857",
            "fdb59b5c3ce443b3b603e42d28f70d11",
            "a6c0380b3d524a76b6befdc470876bc6",
            "429bfdaf4b2d4e228ea1148255d508a3",
            "a9ac664a27e34492893ca00645848135",
            "3909898ae34f476bb58aa2f7452be62a",
            "8618f3d9241f41f5828743de369a7091",
            "7831e685a9da423ea908bc8a7fe2195e",
            "95c083fb60e7441884ac4138d6055b2d",
            "799fb1f0932e496a93de1de8fead5ce5",
            "80e10efc56e7485cbca1265d3e2ca833",
            "68df987a234140fd8fed50dc72d8cac1",
            "4b84e669b6e84f19ad4973145873c123",
            "30cdb25951204561a221acbb929b053f",
            "d3fec8ac703b4504b70156f67352b434",
            "1c1f2de1f1c1404fa680a0d6dd400642"
          ]
        },
        "id": "hNXU66NM58b_",
        "outputId": "9f402a1e-a422-4cdf-b969-d95b1bf43a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Using cached xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94b9d857840c4e289714915796d694a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/5.92k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1994c77e9394fc1b550871c7501f4c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/1.63M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3909898ae34f476bb58aa2f7452be62a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/24783 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Counts in the Dataset:\n",
            "class\n",
            "1    19190\n",
            "2     4163\n",
            "0     1430\n",
            "Name: count, dtype: int64\n",
            "   count  hate_speech_count  offensive_language_count  neither_count  class  \\\n",
            "0      3                  0                         0              3      2   \n",
            "1      3                  0                         3              0      1   \n",
            "2      3                  0                         3              0      1   \n",
            "3      3                  0                         2              1      1   \n",
            "4      6                  0                         6              0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"tdavidson/hate_speech_offensive\")\n",
        "\n",
        "# Convert the Hugging Face Dataset to a pandas DataFrame\n",
        "data = dataset['train'].to_pandas()\n",
        "\n",
        "# Check unique classes in the original dataset\n",
        "class_counts = data['class'].value_counts()\n",
        "print(\"Class Counts in the Dataset:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLofkhc1VDUv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYMx3gL3XXch",
        "outputId": "b2c810f1-ff12-4b5e-a921-23828d7e9da1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data size: 19826\n",
            "Testing data size: 4957\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = data\n",
        "# Split the dataset into 80% training and 20% testing\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Make copies if needed\n",
        "data = train_data.copy()\n",
        "test_data = test_data.copy()\n",
        "\n",
        "print(f\"Training data size: {len(data)}\")\n",
        "print(f\"Testing data size: {len(test_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7jBEC31uaw0"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dD7ctnbYpfZ",
        "outputId": "2f305223-3ae6-4cb4-a026-2d65ae150190"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isnull().values.any()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN8k7QBEZJoM",
        "outputId": "f6e9b57b-1ba9-4a92-d1d2-f4dabe59433d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Function to process text\n",
        "def text_processing(raw_text):\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)  # Remove non-letters\n",
        "    words = letters_only.lower().split()  # Convert to lowercase and split\n",
        "    stops = set(stopwords.words(\"english\"))  # Define stopwords\n",
        "    meaningful_words = [w for w in words if w not in stops]  # Remove stopwords\n",
        "    return \" \".join(meaningful_words)  # Join words back into a single string\n",
        "\n",
        "# Processing text in the training data\n",
        "clean_text = []\n",
        "for text in data[\"tweet\"]:  # Use 'tweet' instead of 'text'\n",
        "    clean_text.append(text_processing(text))\n",
        "data[\"clean_text\"] = clean_text  # Add processed text as a new column\n",
        "\n",
        "# Processing text in the testing data\n",
        "clean_text_test = []\n",
        "for text in test_data[\"tweet\"]:  # Use 'tweet' for the test data as well\n",
        "    clean_text_test.append(text_processing(text))\n",
        "test_data[\"clean_text\"] = clean_text_test  # Add processed text as a new column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwnwi3q8ZN9P",
        "outputId": "39fd4c29-f0a3-4f68-efe0-6f33b1491a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 15860 Testing set size: 3966\n",
            "Classes in training set: class\n",
            "1    12286\n",
            "2     2662\n",
            "0      912\n",
            "Name: count, dtype: int64\n",
            "Classes in testing set: class\n",
            "1    3072\n",
            "2     666\n",
            "0     228\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'class' is the column that contains the labels\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    data[\"clean_text\"],  # Input features\n",
        "    data[\"class\"],       # Output labels\n",
        "    test_size=0.2,      # 20% for testing\n",
        "    random_state=42,    # For reproducibility\n",
        "    stratify=data[\"class\"]  # Ensures proportional representation of classes in train/test sets\n",
        ")\n",
        "\n",
        "# Optional: Print shapes to confirm split\n",
        "print(\"Training set size:\", len(X_train), \"Testing set size:\", len(X_test))\n",
        "print(\"Classes in training set:\", Y_train.value_counts())\n",
        "print(\"Classes in testing set:\", Y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdCYp42zZROv"
      },
      "outputs": [],
      "source": [
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer(\n",
        "    analyzer=\"word\",        # Analyzing individual words\n",
        "    tokenizer=None,         # Using the default tokenization\n",
        "    preprocessor=None,      # No preprocessor, since preprocessing is done already\n",
        "    stop_words=None,        # Using no stop words; they were removed during preprocessing\n",
        "    max_features=5000       # Limiting to 5000 features\n",
        ")\n",
        "\n",
        "# Fit and transform the training data\n",
        "train_data_features = vectorizer.fit_transform(X_train).toarray()\n",
        "\n",
        "# Transform the testing data\n",
        "test_data_features = vectorizer.transform(X_test).toarray()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XDRhyMKZUkJ",
        "outputId": "3d1ed2da-0470-439d-fc52-e0bd1d6271b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training\n",
            "Testing\n",
            "Accuracy:  0.899899142713061\n",
            "Precision: 0.89392337796273, Recall: 0.899899142713061, F1 Score: 0.8965014064946625\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# SVM with linear kernel\n",
        "clf = svm.SVC(kernel='linear', C=1.0)\n",
        "\n",
        "# Training\n",
        "print(\"Training\")\n",
        "clf.fit(train_data_features, Y_train)\n",
        "\n",
        "# Testing\n",
        "print(\"Testing\")\n",
        "predicted = clf.predict(test_data_features)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(predicted == Y_test)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# Calculate additional metrics\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(Y_test, predicted, average='weighted')\n",
        "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGPfu1aFbcbi",
        "outputId": "91b32925-b16f-493d-a760-ba199bf30062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels in training data: ['hate_speech' 'neither' 'offensive_language']\n",
            "Unique labels in test data: ['hate_speech' 'neither' 'offensive_language']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Update the 'class' column for the training data\n",
        "data['class'] = np.where(data['class'] == 1, 'hate_speech',\n",
        "                          np.where(data['class'] == 2, 'offensive_language', 'neither'))\n",
        "\n",
        "# Update the 'class' column for the test data\n",
        "test_data['class'] = np.where(test_data['class'] == 1, 'hate_speech',\n",
        "                               np.where(test_data['class'] == 2, 'offensive_language', 'neither'))\n",
        "\n",
        "# Extract target values for training and testing sets\n",
        "y_train = data['class'].values\n",
        "y_test = test_data['class'].values\n",
        "\n",
        "# Optional: Print to confirm the updates\n",
        "print(\"Unique labels in training data:\", np.unique(y_train))\n",
        "print(\"Unique labels in test data:\", np.unique(y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo0oR-YubfAi",
        "outputId": "3ec57759-695e-44dc-a32d-bd460f9078fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Install NLTK and download the necessary resources\n",
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Define the text column to tokenize\n",
        "text_column = 'tweet'  # Actual column name containing text data\n",
        "\n",
        "# Tokenize the specified text column in the main data and test data\n",
        "data['tokenized_sents'] = data[text_column].apply(lambda x: word_tokenize(str(x)))\n",
        "test_data['tokenized_sents'] = test_data[text_column].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# Check if df exists and tokenize if it does\n",
        "if 'df' in locals():  # Check if df exists\n",
        "    df['tokenized_sents'] = df[text_column].apply(lambda x: word_tokenize(str(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-b9w2ZGbg_w"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Define training data\n",
        "sentences = df['tokenized_sents']\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=200, window=4, min_count=1, sg=1)\n",
        "\n",
        "# Instead of model.wv.vocab, use model.wv.key_to_index to get the vocabulary\n",
        "words = list(model.wv.key_to_index.keys())  # Get keys from key_to_index dictionary\n",
        "\n",
        "# Save the trained model\n",
        "model.save('model.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL03DzNzbjm1",
        "outputId": "e58eda51-44e6-4a26-8ccd-f7c988f0c915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (42410, 200)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Define vocab_size based on the length of the words list\n",
        "vocab_size = len(words)\n",
        "\n",
        "# Initialize the embedding matrix with zeros\n",
        "embedding_matrix = np.zeros((vocab_size, 200))\n",
        "\n",
        "for i in range(vocab_size):  # Iterate through the vocabulary\n",
        "    # Use model.wv[words[i]] to get the embedding vector\n",
        "    embedding_vector = model.wv[words[i]]\n",
        "    embedding_matrix[i] = embedding_vector  # Assign the embedding vector to the matrix\n",
        "\n",
        "# Optionally, print the embedding matrix shape\n",
        "print(\"Embedding matrix shape:\", embedding_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P05CvB3wcjzT",
        "outputId": "7c2bacbd-2b1b-4d7a-f15f-a07d45ed1957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q keras_preprocessing\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYkQrqPNblzL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgONdUr7bpA5",
        "outputId": "bff1bcd4-1c0e-411d-91cc-cb393e817e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (19826, 50)\n",
            "Testing data shape: (4957, 50)\n"
          ]
        }
      ],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the training data's tokenized sentences\n",
        "tokenizer.fit_on_texts(data['tokenized_sents'])\n",
        "sequences = tokenizer.texts_to_sequences(data['tokenized_sents'])\n",
        "X_t = pad_sequences(sequences, maxlen=50)\n",
        "\n",
        "# Define vocab_size based on the tokenizer's word index\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for zero padding\n",
        "\n",
        "# Prepare test data sequences\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data['tokenized_sents'])\n",
        "X_test = pad_sequences(test_sequences, maxlen=50)\n",
        "\n",
        "# Optionally, print the shapes of your padded sequences\n",
        "print(\"Training data shape:\", X_t.shape)\n",
        "print(\"Testing data shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m-1QDeec_CK",
        "outputId": "19168448-cd9e-4e0c-86c5-03c68f50c1a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class Counts in Data:\n",
            "class\n",
            "hate_speech           15358\n",
            "offensive_language     3328\n",
            "neither                1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class Counts in Test Data:\n",
            "class\n",
            "hate_speech           3832\n",
            "offensive_language     835\n",
            "neither                290\n",
            "Name: count, dtype: int64\n",
            "                    class  ratio_hate_speech  ratio_offensive_language  \\\n",
            "15272             neither                0.0                       0.0   \n",
            "9351   offensive_language                0.0                       0.0   \n",
            "20323         hate_speech                0.0                       0.0   \n",
            "3638          hate_speech                0.0                       0.0   \n",
            "20579         hate_speech                0.0                       0.0   \n",
            "...                   ...                ...                       ...   \n",
            "21575  offensive_language                0.0                       0.0   \n",
            "5390          hate_speech                0.0                       0.0   \n",
            "860           hate_speech                0.0                       0.0   \n",
            "15795         hate_speech                0.0                       0.0   \n",
            "23654         hate_speech                0.0                       0.0   \n",
            "\n",
            "       ratio_neither  \n",
            "15272            0.0  \n",
            "9351             0.0  \n",
            "20323            0.0  \n",
            "3638             0.0  \n",
            "20579            0.0  \n",
            "...              ...  \n",
            "21575            0.0  \n",
            "5390             0.0  \n",
            "860              0.0  \n",
            "15795            0.0  \n",
            "23654            0.0  \n",
            "\n",
            "[19826 rows x 4 columns]\n",
            "                    class  ratio_hate_speech  ratio_offensive_language  \\\n",
            "2281          hate_speech                0.0                       0.0   \n",
            "15914         hate_speech                0.0                       0.0   \n",
            "18943  offensive_language                0.0                       0.0   \n",
            "16407         hate_speech                0.0                       0.0   \n",
            "13326         hate_speech                0.0                       0.0   \n",
            "...                   ...                ...                       ...   \n",
            "3310              neither                0.0                       0.0   \n",
            "22759         hate_speech                0.0                       0.0   \n",
            "21953         hate_speech                0.0                       0.0   \n",
            "9056          hate_speech                0.0                       0.0   \n",
            "13246         hate_speech                0.0                       0.0   \n",
            "\n",
            "       ratio_neither  \n",
            "2281             0.0  \n",
            "15914            0.0  \n",
            "18943            0.0  \n",
            "16407            0.0  \n",
            "13326            0.0  \n",
            "...              ...  \n",
            "3310             0.0  \n",
            "22759            0.0  \n",
            "21953            0.0  \n",
            "9056             0.0  \n",
            "13246            0.0  \n",
            "\n",
            "[4957 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "# Count occurrences of each class in the data DataFrame\n",
        "class_counts = data['class'].value_counts()\n",
        "\n",
        "# Count occurrences of each class in the test_data DataFrame\n",
        "class_counts_test = test_data['class'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(\"Class Counts in Data:\")\n",
        "print(class_counts)\n",
        "\n",
        "print(\"\\nClass Counts in Test Data:\")\n",
        "print(class_counts_test)\n",
        "\n",
        "# Get the total counts of classes\n",
        "count1 = class_counts.tolist()  # Convert the counts to a list\n",
        "count_test = class_counts_test.tolist()  # Convert the counts to a list\n",
        "\n",
        "# Initialize ratios for data\n",
        "total_data = len(data)\n",
        "ratio_hate_speech = (data['class'] == 1).sum() / total_data if total_data > 0 else 0\n",
        "ratio_offensive_language = (data['class'] == 2).sum() / total_data if total_data > 0 else 0\n",
        "ratio_neither = (data['class'] == 0).sum() / total_data if total_data > 0 else 0\n",
        "\n",
        "# Create columns for ratios in the original DataFrame\n",
        "data['ratio_hate_speech'] = ratio_hate_speech\n",
        "data['ratio_offensive_language'] = ratio_offensive_language\n",
        "data['ratio_neither'] = ratio_neither\n",
        "\n",
        "# For test_data\n",
        "total_test_data = len(test_data)\n",
        "ratio_hate_speech_test = (test_data['class'] == 1).sum() / total_test_data if total_test_data > 0 else 0\n",
        "ratio_offensive_language_test = (test_data['class'] == 2).sum() / total_test_data if total_test_data > 0 else 0\n",
        "ratio_neither_test = (test_data['class'] == 0).sum() / total_test_data if total_test_data > 0 else 0\n",
        "\n",
        "# Create columns for ratios in the test DataFrame\n",
        "test_data['ratio_hate_speech'] = ratio_hate_speech_test\n",
        "test_data['ratio_offensive_language'] = ratio_offensive_language_test\n",
        "test_data['ratio_neither'] = ratio_neither_test\n",
        "\n",
        "# Optional: Print the updated DataFrames to check the new columns\n",
        "print(data[['class', 'ratio_hate_speech', 'ratio_offensive_language', 'ratio_neither']])\n",
        "print(test_data[['class', 'ratio_hate_speech', 'ratio_offensive_language', 'ratio_neither']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLv43bOEdaSC",
        "outputId": "067215f9-0f44-4bac-e5f0-5bdf211556e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label Counts in Data:\n",
            "class\n",
            "hate_speech           15358\n",
            "offensive_language     3328\n",
            "neither                1140\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label Counts in Test Data:\n",
            "class\n",
            "hate_speech           3832\n",
            "offensive_language     835\n",
            "neither                290\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Data DataFrame with Ratios:\n",
            "                    class  ratio_positive  ratio_negative  ratio_offensive\n",
            "15272             neither             0.0             0.0              0.0\n",
            "9351   offensive_language             0.0             0.0              0.0\n",
            "20323         hate_speech             0.0             0.0              0.0\n",
            "3638          hate_speech             0.0             0.0              0.0\n",
            "20579         hate_speech             0.0             0.0              0.0\n",
            "...                   ...             ...             ...              ...\n",
            "21575  offensive_language             0.0             0.0              0.0\n",
            "5390          hate_speech             0.0             0.0              0.0\n",
            "860           hate_speech             0.0             0.0              0.0\n",
            "15795         hate_speech             0.0             0.0              0.0\n",
            "23654         hate_speech             0.0             0.0              0.0\n",
            "\n",
            "[19826 rows x 4 columns]\n",
            "\n",
            "Test DataFrame with Ratios:\n",
            "                    class  ratio_positive  ratio_negative  ratio_offensive\n",
            "2281          hate_speech             0.0             0.0              0.0\n",
            "15914         hate_speech             0.0             0.0              0.0\n",
            "18943  offensive_language             0.0             0.0              0.0\n",
            "16407         hate_speech             0.0             0.0              0.0\n",
            "13326         hate_speech             0.0             0.0              0.0\n",
            "...                   ...             ...             ...              ...\n",
            "3310              neither             0.0             0.0              0.0\n",
            "22759         hate_speech             0.0             0.0              0.0\n",
            "21953         hate_speech             0.0             0.0              0.0\n",
            "9056          hate_speech             0.0             0.0              0.0\n",
            "13246         hate_speech             0.0             0.0              0.0\n",
            "\n",
            "[4957 rows x 4 columns]\n",
            "\n",
            "Data DataFrame with Tendencies:\n",
            "                    class  tendency_positive  tendency_negative  \\\n",
            "15272             neither                0.0                0.0   \n",
            "9351   offensive_language                0.0                0.0   \n",
            "20323         hate_speech                0.0                0.0   \n",
            "3638          hate_speech                0.0                0.0   \n",
            "20579         hate_speech                0.0                0.0   \n",
            "...                   ...                ...                ...   \n",
            "21575  offensive_language                0.0                0.0   \n",
            "5390          hate_speech                0.0                0.0   \n",
            "860           hate_speech                0.0                0.0   \n",
            "15795         hate_speech                0.0                0.0   \n",
            "23654         hate_speech                0.0                0.0   \n",
            "\n",
            "       tendency_offensive  \n",
            "15272                 0.0  \n",
            "9351                  0.0  \n",
            "20323                 0.0  \n",
            "3638                  0.0  \n",
            "20579                 0.0  \n",
            "...                   ...  \n",
            "21575                 0.0  \n",
            "5390                  0.0  \n",
            "860                   0.0  \n",
            "15795                 0.0  \n",
            "23654                 0.0  \n",
            "\n",
            "[19826 rows x 4 columns]\n",
            "\n",
            "Test DataFrame with Tendencies:\n",
            "                    class  tendency_positive  tendency_negative  \\\n",
            "2281          hate_speech                0.0                0.0   \n",
            "15914         hate_speech                0.0                0.0   \n",
            "18943  offensive_language                0.0                0.0   \n",
            "16407         hate_speech                0.0                0.0   \n",
            "13326         hate_speech                0.0                0.0   \n",
            "...                   ...                ...                ...   \n",
            "3310              neither                0.0                0.0   \n",
            "22759         hate_speech                0.0                0.0   \n",
            "21953         hate_speech                0.0                0.0   \n",
            "9056          hate_speech                0.0                0.0   \n",
            "13246         hate_speech                0.0                0.0   \n",
            "\n",
            "       tendency_offensive  \n",
            "2281                  0.0  \n",
            "15914                 0.0  \n",
            "18943                 0.0  \n",
            "16407                 0.0  \n",
            "13326                 0.0  \n",
            "...                   ...  \n",
            "3310                  0.0  \n",
            "22759                 0.0  \n",
            "21953                 0.0  \n",
            "9056                  0.0  \n",
            "13246                 0.0  \n",
            "\n",
            "[4957 rows x 4 columns]\n",
            "Shape of Training Data with Tendencies: (19826, 54)\n",
            "Shape of Test Data with Tendencies: (4957, 54)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'data' and 'test_data' are your DataFrames\n",
        "\n",
        "# Count occurrences of each label in the data DataFrame\n",
        "label_counts = data['class'].value_counts()\n",
        "label_counts_test = test_data['class'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(\"Label Counts in Data:\")\n",
        "print(label_counts)\n",
        "\n",
        "print(\"\\nLabel Counts in Test Data:\")\n",
        "print(label_counts_test)\n",
        "\n",
        "# Get the total counts of labels\n",
        "count1 = label_counts.tolist() if not label_counts.empty else [0, 0, 0]  # Handle empty case\n",
        "count_test = label_counts_test.tolist() if not label_counts_test.empty else [0, 0, 0]  # Handle empty case\n",
        "\n",
        "# Initialize ratios for data\n",
        "total_data = len(data)\n",
        "ratio_positive = np.zeros(total_data)\n",
        "ratio_negative = np.zeros(total_data)\n",
        "ratio_offensive = np.zeros(total_data)\n",
        "\n",
        "# Calculate ratios for the data DataFrame\n",
        "if total_data > 0:  # Only calculate if data is not empty\n",
        "    ratio_positive = (data['class'] == 1).mean()\n",
        "    ratio_negative = (data['class'] == 0).mean()\n",
        "    ratio_offensive = (data['class'] == 2).mean()\n",
        "\n",
        "    # Create columns for ratios in the original DataFrame\n",
        "    data['ratio_positive'] = ratio_positive\n",
        "    data['ratio_negative'] = ratio_negative\n",
        "    data['ratio_offensive'] = ratio_offensive\n",
        "\n",
        "# For test_data\n",
        "total_test_data = len(test_data)\n",
        "ratio_positive_test = np.zeros(total_test_data)\n",
        "ratio_negative_test = np.zeros(total_test_data)\n",
        "ratio_offensive_test = np.zeros(total_test_data)\n",
        "\n",
        "# Calculate ratios for the test_data DataFrame\n",
        "if total_test_data > 0:  # Only calculate if test_data is not empty\n",
        "    ratio_positive_test = (test_data['class'] == 1).mean()\n",
        "    ratio_negative_test = (test_data['class'] == 0).mean()\n",
        "    ratio_offensive_test = (test_data['class'] == 2).mean()\n",
        "\n",
        "    # Create columns for ratios in the test DataFrame\n",
        "    test_data['ratio_positive'] = ratio_positive_test\n",
        "    test_data['ratio_negative'] = ratio_negative_test\n",
        "    test_data['ratio_offensive'] = ratio_offensive_test\n",
        "\n",
        "# Optional: Print the updated DataFrames to check the new columns\n",
        "print(\"\\nData DataFrame with Ratios:\")\n",
        "print(data[['class', 'ratio_positive', 'ratio_negative', 'ratio_offensive']])\n",
        "\n",
        "print(\"\\nTest DataFrame with Ratios:\")\n",
        "print(test_data[['class', 'ratio_positive', 'ratio_negative', 'ratio_offensive']])\n",
        "\n",
        "# Initialize tendency columns based on labels for the training data\n",
        "if total_data > 0:  # Ensure there is data\n",
        "    data['tendency_positive'] = (data['class'] == 1).mean()  # Overall tendency for 1s (positive)\n",
        "    data['tendency_negative'] = (data['class'] == 0).mean()  # Overall tendency for 0s (negative)\n",
        "    data['tendency_offensive'] = (data['class'] == 2).mean()  # Overall tendency for 2s (offensive)\n",
        "\n",
        "# For the test_data\n",
        "if total_test_data > 0:  # Ensure there is test data\n",
        "    test_data['tendency_positive'] = (test_data['class'] == 1).mean()  # Overall tendency for 1s (positive)\n",
        "    test_data['tendency_negative'] = (test_data['class'] == 0).mean()  # Overall tendency for 0s (negative)\n",
        "    test_data['tendency_offensive'] = (test_data['class'] == 2).mean()  # Overall tendency for 2s (offensive)\n",
        "\n",
        "# Optional: Print the updated DataFrames to check the new tendency columns\n",
        "print(\"\\nData DataFrame with Tendencies:\")\n",
        "print(data[['class', 'tendency_positive', 'tendency_negative', 'tendency_offensive']])\n",
        "\n",
        "print(\"\\nTest DataFrame with Tendencies:\")\n",
        "print(test_data[['class', 'tendency_positive', 'tendency_negative', 'tendency_offensive']])\n",
        "\n",
        "# Now let's assume you have your features ready, and you want to concatenate the datasets\n",
        "# Example feature arrays (replace with your actual feature arrays)\n",
        "# X_t should have shape (total_data, number_of_features)\n",
        "# tendency_hate and tendency_no_hate should have shape (total_data, 1)\n",
        "\n",
        "X_t = np.random.rand(total_data, 51)  # Replace with your actual features\n",
        "tendency_hate = data[['tendency_positive']].values  # Shape (total_data, 1)\n",
        "tendency_no_hate = data[['tendency_negative']].values  # Shape (total_data, 1)\n",
        "tendency_offensive = data[['tendency_offensive']].values  # Shape (total_data, 1)\n",
        "\n",
        "# Concatenate features for the training data\n",
        "X1_t = np.concatenate((X_t, tendency_hate, tendency_no_hate, tendency_offensive), axis=1)\n",
        "print(\"Shape of Training Data with Tendencies:\", X1_t.shape)  # Should be (total_data, 54)\n",
        "\n",
        "# For the test data\n",
        "# Assuming you have similar features for test_data\n",
        "X_test = np.random.rand(total_test_data, 51)  # Replace with your actual test features\n",
        "tendency_hate_test = test_data[['tendency_positive']].values  # Shape (total_test_data, 1)\n",
        "tendency_no_hate_test = test_data[['tendency_negative']].values  # Shape (total_test_data, 1)\n",
        "tendency_offensive_test = test_data[['tendency_offensive']].values  # Shape (total_test_data, 1)\n",
        "\n",
        "# Concatenate features for the test data\n",
        "X1_test = np.concatenate((X_test, tendency_hate_test, tendency_no_hate_test, tendency_offensive_test), axis=1)\n",
        "print(\"Shape of Test Data with Tendencies:\", X1_test.shape)  # Should be (total_test_data, 54) if not empty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hzo5jT6e3L4",
        "outputId": "164c6c1d-79e2-4edb-b6ec-47df7cfc227e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.39.0 watchdog-5.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubti3vHkFW-f",
        "outputId": "db025714-55c5-41c7-f457-7be430cc63b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 15860 Testing set size: 3966\n",
            "Classes in training set: class\n",
            "hate_speech           12286\n",
            "offensive_language     2662\n",
            "neither                 912\n",
            "Name: count, dtype: int64\n",
            "Classes in testing set: class\n",
            "hate_speech           3072\n",
            "offensive_language     666\n",
            "neither                228\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'data' is your DataFrame containing 'clean_text' and 'class'\n",
        "# Split the data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    data[\"clean_text\"],\n",
        "    data[\"class\"],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=data[\"class\"]\n",
        ")\n",
        "\n",
        "# Optional: Print shapes to confirm split\n",
        "print(\"Training set size:\", len(X_train), \"Testing set size:\", len(X_test))\n",
        "print(\"Classes in training set:\", Y_train.value_counts())\n",
        "print(\"Classes in testing set:\", Y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX60yxQDY8ru",
        "outputId": "57e263b3-4173-4e89-b875-41aef3fd6a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1KXBbuSwzJoYA5ZtWspHYHZFW0Q3pHN4r\n",
            "From (redirected): https://drive.google.com/uc?id=1KXBbuSwzJoYA5ZtWspHYHZFW0Q3pHN4r&confirm=t&uuid=80de2859-303f-4595-aa57-0a8e2f4e365b\n",
            "To: /content/glove.840B.300d.txt\n",
            "100% 5.65G/5.65G [01:23<00:00, 67.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Install gdown if you haven't already\n",
        "!pip install gdown\n",
        "\n",
        "# Use gdown to download the GloVe embeddings\n",
        "!gdown 'https://drive.google.com/uc?id=1KXBbuSwzJoYA5ZtWspHYHZFW0Q3pHN4r'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WckwwLuncRNb",
        "outputId": "e7e2543c-da4c-4fc4-f344-c124714ed2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.40228 0.0027747 -0.3311 -0.30691 2.0817 0.031819 0.013643 0.30265 0.0071297 -0.5819 -0.2774 -0.062254 1.1451 -0.24232 0.1235 -0.12243 0.33152 -0.006162 -0.30541 -0.13057 -0.054601 0.037083 -0.070552 0.5893 -0.30385 0.2898 -0.14653 -0.27052 0.37161 0.32031 -0.29125 0.0052483 -0.13212 -0.052736 0.087349 -0.26668 -0.16897 0.015162 -0.0083746 -0.14871 0.23413 -0.20719 -0.091386 0.40075 -0.17223 0.18145 0.37586 -0.28682 0.37289 -0.16185 0.18008 0.3032 -0.13216 0.18352 0.095759 0.094916 0.008289 0.11761 0.34046 0.03677 -0.29077 0.058303 -0.027814 0.082941 0.1862 -0.031494 0.27985 -0.074412 -0.13762 -0.21866 0.18138 0.040855 -0.113 0.24107 0.3657 -0.27525 -0.05684 0.34872 0.011884 0.14517 -0.71395 0.48497 0.14807 0.62287 0.20599 0.58379 -0.13438 0.40207 0.18311 0.28021 -0.42349 -0.25626 0.17715 -0.54095 0.16596 -0.036058 0.08499 -0.64989 0.075549 -0.28831 0.40626 -0.2802 0.094062 0.32406 0.28437 -0.26341 0.11553 0.071918 -0.47215 -0.18366 -0.34709 0.29964 -0.66514 0.002516 -0.42333 0.27512 0.36012 0.16311 0.23964 -0.05923 0.3261 0.20559 0.038677 -0.045816 0.089764 0.43151 -0.15954 0.08532 -0.26572 -0.15001 0.084286 -0.16714 -0.43004 0.060807 0.13121 -0.24112 0.66554 0.4453 -0.18019 -0.13919 0.56252 0.21457 -0.46443 -0.012211 0.029988 -0.051094 -0.20135 0.80788 0.47377 -0.057647 0.46216 0.16084 -0.20954 -0.05452 0.15572 -0.13712 0.12972 -0.011936 -0.003378 -0.13595 -0.080711 0.20065 0.054056 0.046816 0.059539 0.046265 0.17754 -0.31094 0.28119 -0.24355 0.085252 -0.21011 -0.19472 0.0027297 -0.46341 0.14789 -0.31517 -0.065939 0.036106 0.42903 -0.33759 0.16432 0.32568 -0.050392 -0.054297 0.24074 0.41923 0.13012 -0.17167 -0.37808 -0.23089 -0.019477 -0.29291 -0.30824 0.30297 -0.22659 0.081574 -0.18516 -0.21408 0.40616 -0.28974 0.074174 -0.17795 0.28595 -0.039626 -0.2339 -0.36054 -0.067503 -0.091065 0.23438 -0.0041331 0.003232 0.0072134 0.008697 0.21614 0.049904 0.35582 0.13748 0.073361 0.14166 0.2412 -0.013322 0.15613 0.083381 0.088146 -0.019357 0.43795 0.083961 0.45309 -0.50489 -0.10865 -0.2527 -0.18251 0.20441 0.13319 0.1294 0.050594 -0.15612 -0.39543 0.12538 0.24881 -0.1927 -0.31847 -0.12719 0.4341 0.31177 -0.0040946 -0.2094 -0.079961 0.1161 -0.050794 0.015266 -0.2803 -0.12486 0.23587 0.2339 -0.14023 0.028462 0.56923 -0.1649 -0.036429 0.010051 -0.17107 -0.042608 0.044965 -0.4393 -0.26137 0.30088 -0.060772 -0.45312 -0.19076 -0.20288 0.27694 -0.060888 0.11944 0.62206 -0.19343 0.47849 -0.30113 0.059389 0.074901 0.061068 -0.4662 0.40054 -0.19099 -0.14331 0.018267 -0.18643 0.20709 -0.35598 0.05338 -0.050821 -0.1918 -0.37846 -0.06589\n",
            "\n",
            ". 0.012001 0.20751 -0.12578 -0.59325 0.12525 0.15975 0.13748 -0.33157 -0.13694 1.7893 -0.47094 0.70434 0.26673 -0.089961 -0.18168 0.067226 0.053347 1.5595 -0.2541 0.038413 -0.01409 0.056774 0.023434 0.024042 0.31703 0.19025 -0.37505 0.035603 0.1181 0.012032 -0.037566 -0.5046 -0.049261 0.092351 0.11031 -0.073062 0.33994 0.28239 0.13413 0.070128 -0.022099 -0.28103 0.49607 -0.48693 -0.090964 -0.1538 -0.38011 -0.014228 -0.19392 -0.11068 -0.014088 -0.17906 0.24509 -0.16878 -0.15351 -0.13808 0.02151 0.13699 0.0068061 -0.14915 -0.38169 0.12727 0.44007 0.32678 -0.46117 0.068687 0.34747 0.18827 -0.31837 0.4447 -0.2095 -0.26987 0.48945 0.15388 0.05295 -0.049831 0.11207 0.14881 -0.37003 0.30777 -0.33865 0.045149 -0.18987 0.26634 -0.26401 -0.47556 0.68381 -0.30653 0.24606 0.31611 -0.071098 0.030417 0.088119 0.045025 0.20125 -0.21618 -0.36371 -0.25948 -0.42398 -0.14305 -0.10208 0.21498 -0.21924 -0.17935 0.21546 0.13801 0.24504 -0.2559 0.054815 0.21307 0.2564 -0.25673 0.17961 -0.47638 -0.25181 -0.0091498 -0.054362 -0.21007 0.12597 -0.40795 -0.021164 0.20585 0.18925 -0.0051896 -0.51394 0.28862 -0.077748 -0.27676 0.46567 -0.14225 -0.17879 -0.4357 -0.32481 0.15034 -0.058367 0.49652 0.20472 0.019866 0.13326 0.12823 -1.0177 0.29007 0.28995 0.029994 -0.10763 0.28665 -0.24387 0.22905 -0.26249 -0.069269 -0.17889 0.21936 0.15146 0.04567 -0.050497 0.071482 -0.1027 -0.080705 0.30296 0.031302 0.26613 -0.0060951 0.10313 -0.39987 -0.043945 -0.057625 0.08702 -0.098152 0.22835 -0.005211 0.038075 0.01591 -0.20622 0.021853 0.0040426 -0.043063 -0.002294 -0.26097 -0.25802 -0.28158 -0.23118 -0.010404 -0.30102 -0.4042 0.014653 -0.10445 0.30377 -0.20957 0.3119 0.068272 0.1008 0.010423 0.54011 0.29865 0.12653 0.013761 0.21738 -0.39521 0.066633 0.50327 0.14913 -0.11554 0.010042 0.095698 0.16607 -0.18808 0.055019 0.026715 -0.3164 -0.046583 -0.051591 0.023475 -0.11007 0.085642 0.28394 0.040497 0.071986 0.14157 -0.021199 0.44718 0.20088 -0.12964 -0.067183 0.47614 0.13394 -0.17287 -0.37324 -0.17285 0.02683 -0.1316 0.09116 -0.46487 0.1274 -0.090159 -0.10552 0.068006 -0.13381 0.17056 0.089509 -0.23133 -0.27572 0.061534 -0.051646 0.28377 0.25286 -0.24139 -0.19905 0.12049 -0.1011 0.27392 0.27843 0.26449 -0.18292 -0.048961 0.19198 0.17192 0.33659 -0.20184 -0.34305 -0.24553 -0.15399 0.3945 0.22839 -0.25753 -0.25675 -0.37332 -0.23884 -0.048816 0.78323 0.18851 -0.26477 0.096566 0.062658 -0.30668 -0.43334 0.10006 0.21136 0.039459 -0.11077 0.24421 0.60942 -0.46646 0.086385 -0.39702 -0.23363 0.021307 -0.10778 -0.2281 0.50803 0.11567 0.16165 -0.066737 -0.29556 0.022612 -0.28135 0.0635 0.14019 0.13871 -0.36049 -0.035\n",
            "\n",
            "the 0.27204 -0.06203 -0.1884 0.023225 -0.018158 0.0067192 -0.13877 0.17708 0.17709 2.5882 -0.35179 -0.17312 0.43285 -0.10708 0.15006 -0.19982 -0.19093 1.1871 -0.16207 -0.23538 0.003664 -0.19156 -0.085662 0.039199 -0.066449 -0.04209 -0.19122 0.011679 -0.37138 0.21886 0.0011423 0.4319 -0.14205 0.38059 0.30654 0.020167 -0.18316 -0.0065186 -0.0080549 -0.12063 0.027507 0.29839 -0.22896 -0.22882 0.14671 -0.076301 -0.1268 -0.0066651 -0.052795 0.14258 0.1561 0.05551 -0.16149 0.09629 -0.076533 -0.049971 -0.010195 -0.047641 -0.16679 -0.2394 0.0050141 -0.049175 0.013338 0.41923 -0.10104 0.015111 -0.077706 -0.13471 0.119 0.10802 0.21061 -0.051904 0.18527 0.17856 0.041293 -0.014385 -0.082567 -0.035483 -0.076173 -0.045367 0.089281 0.33672 -0.22099 -0.0067275 0.23983 -0.23147 -0.88592 0.091297 -0.012123 0.013233 -0.25799 -0.02972 0.016754 0.01369 0.32377 0.039546 0.042114 -0.088243 0.30318 0.087747 0.16346 -0.40485 -0.043845 -0.040697 0.20936 -0.77795 0.2997 0.2334 0.14891 -0.39037 -0.053086 0.062922 0.065663 -0.13906 0.094193 0.10344 -0.2797 0.28905 -0.32161 0.020687 0.063254 -0.23257 -0.4352 -0.017049 -0.32744 -0.047064 -0.075149 -0.18788 -0.015017 0.029342 -0.3527 -0.044278 -0.13507 -0.11644 -0.1043 0.1392 0.0039199 0.37603 0.067217 -0.37992 -1.1241 -0.057357 -0.16826 0.03941 0.2604 -0.023866 0.17963 0.13553 0.2139 0.052633 -0.25033 -0.11307 0.22234 0.066597 -0.11161 0.062438 -0.27972 0.19878 -0.36262 -1.0006e-05 -0.17262 0.29166 -0.15723 0.054295 0.06101 -0.39165 0.2766 0.057816 0.39709 0.025229 0.24672 -0.08905 0.15683 -0.2096 -0.22196 0.052394 -0.01136 0.050417 -0.14023 -0.042825 -0.031931 -0.21336 -0.20402 -0.23272 0.07449 0.088202 -0.11063 -0.33526 -0.014028 -0.29429 -0.086911 -0.1321 -0.43616 0.20513 0.0079362 0.48505 0.064237 0.14261 -0.43711 0.12783 -0.13111 0.24673 -0.27496 0.15896 0.43314 0.090286 0.24662 0.066463 -0.20099 0.1101 0.03644 0.17359 -0.15689 -0.086328 -0.17316 0.36975 -0.40317 -0.064814 -0.034166 -0.013773 0.062854 -0.17183 -0.12366 -0.034663 -0.22793 -0.23172 0.239 0.27473 0.15332 0.10661 -0.060982 -0.024805 -0.13478 0.17932 -0.37374 -0.02893 -0.11142 -0.08389 -0.055932 0.068039 -0.10783 0.1465 0.094617 -0.084554 0.067429 -0.3291 0.034082 -0.16747 -0.25997 -0.22917 0.020159 -0.02758 0.16136 -0.18538 0.037665 0.57603 0.20684 0.27941 0.16477 -0.018769 0.12062 0.069648 0.059022 -0.23154 0.24095 -0.3471 0.04854 -0.056502 0.41566 -0.43194 0.4823 -0.051759 -0.27285 -0.25893 0.16555 -0.1831 -0.06734 0.42457 0.010346 0.14237 0.25939 0.17123 -0.13821 -0.066846 0.015981 -0.30193 0.043579 -0.043102 0.35025 -0.19681 -0.4281 0.16899 0.22511 -0.28557 -0.1028 -0.018168 0.11407 0.13015 -0.18317 0.1323\n",
            "\n",
            "and -0.18567 0.066008 -0.25209 -0.11725 0.26513 0.064908 0.12291 -0.093979 0.024321 2.4926 -0.017916 -0.071218 -0.24782 -0.26237 -0.2246 -0.21961 -0.12927 1.0867 -0.66072 -0.031617 -0.057328 0.056903 -0.27939 -0.39825 0.14251 -0.085146 -0.14779 0.055067 -0.0028687 -0.20917 -0.070735 0.22577 -0.15881 -0.10395 0.09711 -0.56251 -0.32929 -0.20853 0.0098711 0.049777 0.0014883 0.15884 0.042771 -0.0026956 -0.02462 -0.19213 -0.22556 0.10838 0.090086 -0.13291 0.32559 -0.17038 -0.1099 -0.23986 -0.024289 0.014656 -0.237 0.084828 -0.35982 -0.076746 0.048909 0.11431 -0.21013 0.24765 -0.017531 -0.14028 0.046191 0.22972 0.1175 0.12724 0.012992 0.4587 0.41085 0.039106 0.15713 -0.18376 0.26834 0.056662 0.16844 -0.053788 -0.091892 0.11193 -0.08681 -0.13324 0.15062 -0.31733 -0.22078 0.25038 0.34131 0.36419 -0.089514 -0.22193 0.24471 0.040091 0.47798 -0.029996 0.0019212 0.063511 -0.20417 -0.26478 0.20649 0.015573 -0.27722 -0.18861 -0.10289 -0.49773 0.14986 -0.010877 0.25085 -0.28117 0.18966 -0.065879 0.094753 -0.15338 -0.055071 -0.36747 0.24993 0.096527 0.23538 0.18405 0.052859 0.22967 0.12582 0.15536 -0.17275 0.33946 -0.10049 0.074948 -0.093575 -0.04049 -0.016922 -0.0058039 -0.18108 0.19537 0.45178 0.10965 0.2337 -0.09905 -0.078633 0.21678 -0.71231 -0.099759 0.33333 -0.1646 -0.091688 0.21056 0.023669 0.028922 0.1199 -0.12512 -0.026037 -0.062217 0.55816 0.0050273 -0.30888 0.038611 0.17568 -0.11163 -0.10815 -0.19444 0.29433 0.14519 -0.042878 0.18534 0.018891 -0.61883 0.13352 0.036007 0.33995 0.22109 -0.079328 0.071319 0.17678 0.16378 -0.23142 -0.1434 -0.098122 -0.019286 0.2356 -0.34013 -0.061007 -0.23208 -0.31152 0.10063 -0.15957 0.20183 -0.016345 -0.12303 0.022667 -0.20986 -0.20127 -0.087883 0.064731 0.10195 -0.1786 0.33056 0.21407 -0.32165 -0.17106 0.19407 -0.38618 -0.2148 -0.052254 0.023175 0.47389 0.18612 0.12711 0.20855 -0.10256 -0.12016 -0.40488 0.029695 -0.027419 -0.0085227 -0.11415 0.081134 -0.17228 0.19142 0.026514 0.043789 -0.12399 0.13354 0.10112 0.081682 -0.15085 0.0075806 -0.18971 0.24669 0.22491 0.35553 -0.3277 -0.21821 0.1402 0.28604 0.055226 -0.086544 0.02111 -0.19236 0.074245 0.076782 0.00081666 0.034097 -0.57719 0.10657 0.28134 -0.11964 -0.68281 -0.32893 -0.24442 -0.025847 0.0091273 0.2025 -0.050959 -0.11042 0.010962 0.076773 0.40048 -0.40739 -0.44773 0.31954 -0.036326 -0.012789 -0.17282 0.1476 0.2356 0.080642 -0.36528 -0.0083443 0.6239 -0.24379 0.019917 -0.28803 -0.010494 0.038412 -0.11718 -0.072462 0.16381 0.38488 -0.029783 0.23444 0.4532 0.14815 -0.027021 -0.073181 -0.1147 -0.0054545 0.47796 0.090912 0.094489 -0.36882 -0.59396 -0.097729 0.20072 0.17055 -0.0047356 -0.039709 0.32498 -0.023452 0.12302 0.3312\n",
            "\n",
            "to 0.31924 0.06316 -0.27858 0.2612 0.079248 -0.21462 -0.10495 0.15495 -0.03353 2.4834 -0.50904 0.08749 0.21426 0.22151 -0.25234 -0.097544 -0.1927 1.3606 -0.11592 -0.10383 0.21929 0.11997 -0.11063 0.14212 -0.16643 0.21815 0.0042086 -0.070012 -0.23532 -0.26518 0.031248 0.16669 -0.089777 0.20059 0.31614 -0.5583 0.075735 0.27635 0.12741 -0.18185 -0.12722 0.024686 -0.077233 -0.48998 0.020355 0.0039164 0.1215 0.089723 -0.078975 0.081443 -0.099087 -0.055621 0.10737 -0.0044042 0.48496 0.11717 -0.017329 0.109 -0.35558 0.051084 0.15714 0.17961 -0.29711 0.033645 -0.025792 -0.013931 -0.23 -0.040306 0.22282 -0.013544 0.011554 0.3911 0.26533 -0.31012 0.40539 -0.042975 0.020811 -0.33033 0.19573 -0.037958 0.10274 -0.0013581 -0.44505 0.077886 0.08511 -0.20285 -0.19481 0.056933 0.53105 0.034154 -0.56996 -0.18469 0.093403 0.28044 -0.23349 0.10938 -0.014288 -0.274 0.034196 -0.098479 0.13268 0.19437 0.13463 -0.099059 0.040324 -0.66272 0.3571 0.15429 0.18598 0.087542 0.080538 -0.25121 0.24155 0.1783 0.036011 -0.027677 0.21161 -0.29107 -0.0083456 0.11317 0.31064 -0.10693 -0.27367 -0.039785 0.039881 0.034462 -0.16518 0.16115 0.060826 0.3075 -0.22398 0.14619 -0.2661 0.49732 -0.13996 -0.24287 0.039469 -0.084495 -0.24315 0.070701 -1.0136 -0.21733 -0.36878 -0.24973 0.17472 -0.011592 0.068561 -0.090411 0.21878 -0.2639 0.11904 0.14285 -0.18707 -0.13474 -0.13232 -0.26553 0.22947 -0.018215 0.0067383 -0.1019 0.10053 -0.1127 -0.13295 0.15951 0.14906 -0.095578 0.26992 0.011057 0.056568 0.021386 0.20215 0.00048589 0.5336 -0.22947 0.29275 0.17378 0.25423 -0.10976 0.058816 0.014616 -0.04306 0.10732 -0.028149 -0.19181 0.1025 -0.063892 0.012737 -0.12913 0.015037 0.26562 -0.017049 -0.060716 -0.094919 0.017775 0.13221 0.1683 -0.19323 -0.17612 0.075506 0.18939 0.12508 -0.1988 -0.16017 -0.21092 0.46933 0.044747 0.098349 0.011637 0.22281 -0.010837 -0.04833 -0.47335 -0.36811 -0.13592 -0.15086 0.25416 0.069531 0.14211 -0.26703 -0.1259 0.12076 -0.26117 0.033024 -0.034398 -0.13968 0.13446 -0.16709 0.15002 -0.13724 0.091226 -0.27718 0.020098 0.26919 0.43016 0.094019 -0.085496 -0.25192 -0.11645 -0.039734 0.0046738 0.54178 -0.16636 0.34546 0.098501 0.47819 -0.38428 -0.3238 -0.14822 -0.47817 0.16704 -0.064505 0.11834 -0.3448 0.096891 0.32309 0.41471 0.19463 -0.20891 -0.12223 -0.058298 -0.20268 0.2948 0.043397 0.10112 0.27177 -0.52124 -0.073794 0.044808 0.41388 0.088782 0.62255 -0.072391 0.090129 0.15428 0.023163 -0.13028 0.061762 0.33803 -0.091581 0.21039 0.05108 0.19184 0.10444 0.2138 -0.35091 -0.23702 0.038399 -0.10031 0.18359 0.025178 -0.12977 0.3713 0.18888 -0.0042738 -0.10645 -0.2581 -0.044629 0.082745 0.097801 0.25045\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If running in Google Colab\n",
        "glove_file_path = '/content/glove.840B.300d.txt'\n",
        "\n",
        "# Preview the first few lines of the GloVe file\n",
        "with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "    for _ in range(5):\n",
        "        print(f.readline())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvByPWwQtMzz",
        "outputId": "ae7ddad4-09b0-47f3-e590-f48b1804d61b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping line: .\u00a0.\u00a0. -0.1573 -0.29517 0.30453 -0.54773 0.098293 -0.1776 0.21662 0.19261 -0.21101 0.53788 -0.047755 0.40675 0.023592 -0.32814 0.046858 0.19367 0.25565 -0.021019 -0.15957 -0.1023 0.20303 -0.043333 0.11618 -0.18486 0.0011948 -0.052301 0.34587 0.052335 0.16774 -0.21384 0.055947 0.24934 -0.12179 0.16749 0.28922 -0.033739 0.3015 -0.13241 0.092635 0.37155 -0.2884 -0.0052731 -0.001005 -0.51153 -0.28476 -0.20139 0.11837 -0.0055891 0.43604 0.16796 -0.2701 0.063957 -0.093253 -0.22079 0.36501 0.06545 0.23941 -0.19292 0.098293 0.12172 -0.1168 -0.027436 0.20507 -0.39139 -0.23111 0.46239 0.22888 -0.028415 -0.1798 0.23817 0.28093 -0.47935 0.23177 -0.35587 0.14246 0.11861 0.011018 0.091986 0.0054809 -0.39955 -0.40183 -0.10629 -0.30851 0.12383 -0.16737 -0.43569 0.4211 -0.57416 -0.19964 0.51312 0.090747 -0.21657 0.043519 0.24288 0.081134 0.49104 -0.33342 -0.31056 -0.3136 0.26931 -0.14402 0.33185 -0.21662 -0.072985 0.080603 -0.7266 -0.098385 -0.36233 -0.25346 0.1154 0.25738 0.15802 -0.15633 -0.024581 0.35673 0.31153 0.33475 -0.081155 -0.3061 0.019077 -0.049047 -0.11232 -0.07417 0.35596 -0.2642 0.012781 -0.20715 0.020223 0.054534 -0.28803 0.42863 -0.10312 0.24771 0.013196 0.19768 -0.013528 -0.15134 0.20307 -0.028973 -0.022706 -0.29199 -0.082062 0.19048 0.0053574 0.14067 -0.28675 0.21343 0.42428 -0.28186 -0.11801 -0.45227 -0.0067998 0.044784 -0.0062886 0.25087 0.34481 -0.64459 -0.20467 0.35007 0.1468 -0.14007 -0.0050219 -0.24053 0.41426 -0.40902 0.21141 0.25726 -0.4883 0.027066 0.56367 -0.39594 -0.035206 0.63079 0.14343 0.038315 0.32527 -0.080335 -0.20065 -0.30848 -0.0031591 0.15296 -0.21014 0.42143 -0.20944 -0.069285 0.13555 -0.020401 -0.22555 0.33491 0.16035 0.17739 -0.023627 0.097575 -0.19395 -0.018754 -0.119 -0.0067027 -0.4178 0.29027 0.13034 -0.30212 0.61173 -0.39918 -0.020191 -0.34531 -0.092082 0.46818 0.36671 0.21021 -0.053162 -0.37872 -0.14271 -0.13604 0.31715 -0.17227 -0.091266 0.16417 0.15069 0.53556 -0.29678 0.13965 -0.29788 0.1282 0.1971 -0.045515 -0.41355 -0.050333 -0.39015 -0.29579 -0.096145 -0.03151 0.053714 -0.37309 -0.36523 -0.17235 0.39251 -0.065909 -0.25267 -0.34448 -0.11503 0.43665 0.18832 0.20631 0.27801 -0.046077 0.13397 -0.091953 -0.098542 0.15811 0.2752 0.081383 0.32077 -0.10028 0.1088 -0.24836 0.10477 0.15243 -0.071302 0.12861 0.23061 0.0074864 0.090918 -0.12269 -0.14831 0.010586 0.35745 -0.23412 -0.23746 -0.22646 -0.27641 -0.1634 0.071909 -0.093884 0.21331 -0.20627 0.44406 0.34691 0.019064 0.034657 0.36789 0.32276 -0.31099 -0.023443 -0.77048 -0.26001 0.033961 -0.13874 0.051973 -0.0090509 0.27427 0.046548 -0.48214 -0.1437 -0.1975 -0.038126 -0.16555 0.071697 0.049449 0.15386 -0.81663\n",
            "Skipping line: at\u00a0name@domain.com 0.0061218 0.39595 -0.22079 0.78149 0.38759 0.28888 0.18495 -0.37328 -0.60018 0.19625 0.42975 0.17942 0.06375 -0.44127 0.72035 0.50539 0.17985 -0.71305 0.11122 0.19733 0.063884 0.023288 0.017074 0.04756 -0.083167 0.14506 -0.21856 -0.07979 -0.058909 -0.79864 0.65868 -0.45031 0.41921 -0.043908 -0.059099 0.21384 -0.05214 0.31267 0.20417 -0.66489 -0.17885 -0.13729 -0.13825 -0.59669 -0.09297 0.31121 -0.027161 0.17923 0.43076 0.1012 0.32516 -0.34159 0.21662 -0.31267 -0.065984 -0.35734 -0.52104 -0.16432 -0.31183 0.33793 -0.3952 0.11288 0.10796 0.16632 -0.077719 0.48287 -0.50849 0.37261 0.27273 0.36141 -0.027295 -0.44898 0.48865 0.13509 0.0082683 0.44873 0.46155 -0.28687 0.28691 -0.16943 -0.4255 -0.13788 -0.84983 0.38833 -0.092206 0.09504 0.57431 -0.55836 0.37006 -0.23685 0.31727 -0.29539 0.40986 -0.30945 -0.4857 0.52769 -0.94486 -0.22039 0.1913 0.014544 0.37755 -0.36721 -0.07249 -0.17701 0.32814 0.2158 0.43346 0.14774 -0.59317 -0.23463 -0.064771 -0.42996 0.59558 0.62189 0.62726 -0.97772 -0.15548 -0.25757 0.57236 0.7448 0.33936 0.088019 0.12554 0.44163 0.48089 0.14379 0.44052 0.0033798 0.17334 0.62277 0.13859 -0.46311 -0.3785 -0.09034 -0.31398 0.43096 0.14501 0.10217 -0.30989 -0.20551 0.95437 0.36931 -1.0046 -0.20179 0.2239 0.093057 0.4248 -0.14321 -1.0822 0.49819 0.42587 0.48591 0.22223 0.01698 -0.082398 0.69111 0.3967 -0.50514 0.12603 0.2714 0.7082 -0.079401 -0.10775 -0.48217 0.85182 -0.0047614 0.06633 0.065622 -0.31762 0.14979 -0.48519 0.31868 0.66676 -0.53971 0.26185 -0.025039 -0.042315 0.096601 -0.80947 0.11489 0.22729 0.15132 0.48739 -0.17873 0.86134 0.20061 0.018419 -0.57752 0.31828 0.46305 0.31762 -0.43985 0.56173 0.14558 -0.23908 0.94529 0.35095 0.09231 0.38298 0.18845 0.12514 0.11943 -0.11206 0.17589 0.2973 0.31892 -0.015596 0.32167 0.066616 -0.04847 0.31435 0.12557 0.036991 0.17287 -0.055305 0.87528 0.11223 0.28194 -0.08516 0.58705 0.18693 -0.32145 0.46188 0.51299 -0.88555 -0.46913 -0.26764 -0.51375 -0.38804 -0.78149 -0.14336 -0.25571 0.12985 0.33621 -0.22846 0.48309 -0.47053 -0.23177 0.17509 0.060625 0.19792 -0.10821 -0.090677 -0.18252 0.73394 -0.17721 0.28438 0.75051 0.37784 -0.26525 0.15274 -0.4612 -0.60663 -0.042408 0.093703 -0.59664 0.097915 -0.46943 -0.18268 -0.26516 -0.14962 0.05476 -0.078979 -0.50688 0.39585 0.69243 0.039802 0.13965 0.21709 0.023105 0.19744 0.45611 0.26455 -0.17963 -0.2695 0.16681 0.27557 -0.13141 -0.12941 -0.40996 0.23409 0.40783 0.28148 -0.15247 0.17776 0.077139 0.30951 0.21843 -0.075161 0.33983 0.30904 0.1404 0.5608 -0.020466 0.29512 -0.43178 -0.31083 -0.28874 -0.12015 0.38455\n",
            "Skipping line: .\u00a0.\u00a0.\u00a0.\u00a0. -0.23773 -0.82788 0.82326 -0.91878 0.35868 0.1309 0.26195 -0.30068 0.42963 -1.5335 0.50492 0.59069 0.17763 -0.010302 -0.63371 -0.040832 0.69336 -0.85472 0.693 -0.44824 0.25476 0.17091 0.96056 -0.26516 0.17454 -0.27371 0.20591 -0.60435 -0.50923 0.36049 0.75756 -0.79493 0.12379 0.40019 0.56716 0.19451 0.15996 -0.07882 0.9756 0.54761 -0.25123 -0.70006 -0.78286 -0.95702 0.23045 -0.3643 0.80267 -0.30315 0.35283 -0.19961 0.71134 -0.94116 -0.086543 0.17827 0.40124 -0.071006 0.82202 -0.51055 0.47492 -0.0582 -0.2687 0.94119 0.0066621 -0.53636 -0.68859 0.27149 0.33482 0.53987 -0.72912 -0.53719 0.17591 -0.52643 0.42509 0.12433 -0.17178 0.53168 0.69058 0.703 0.17845 -0.39243 -0.50617 0.39575 0.18461 -0.26065 -0.61119 0.013174 0.32934 -0.4861 -0.46474 0.69677 0.47846 -0.43841 0.051551 0.4195 -0.36847 0.57844 0.053724 -0.026579 -0.5901 0.73221 -0.53987 -0.3428 -0.12343 -0.51213 -0.59237 -0.14589 0.094055 -0.48848 -0.09944 0.092527 0.76764 -0.21207 -0.2395 0.27517 0.27508 0.29604 0.01908 -0.64152 0.51208 0.30948 -0.47126 0.86562 -0.68993 0.19003 -0.78941 -0.10668 -0.64497 0.078153 -0.29598 -0.2383 0.40626 0.066945 0.021937 -0.28291 -0.34962 0.098551 -0.068353 0.06824 -0.17466 -0.21469 1.4612 0.30606 -0.28516 0.078682 0.54627 -0.28879 0.11363 0.32769 -0.38409 -0.038104 -0.70277 -0.47623 -0.29545 0.1906 1.2011 0.38105 0.24595 -0.15847 0.051696 0.71491 0.040783 -0.24121 -0.69558 0.8648 -0.18283 -0.11746 -0.058645 0.033855 -0.32785 -0.23554 -0.12762 0.088837 1.1886 0.6515 -0.32243 0.20434 -0.28283 -0.076 -0.5461 0.66366 -0.19004 0.41687 0.16786 -0.57624 0.0021248 0.038208 0.46581 0.17952 0.15208 0.77239 0.56825 1.3564 0.36707 -0.90959 -0.20369 -0.30854 0.43101 -0.62625 0.62072 -0.48968 -0.4026 0.83121 0.27788 -0.63801 -0.90269 -0.26409 0.55212 -0.21899 0.57153 0.10422 -0.23276 0.32775 0.15975 0.52786 -0.18071 -0.66116 -0.28231 -0.95566 0.32314 -0.10176 0.49961 -0.59512 0.30664 0.17566 -0.0082404 -1.2304 -0.18822 -0.094328 -0.25801 0.07948 0.59872 -0.029741 -0.14526 -0.044699 0.23121 0.20907 -0.442 0.40599 0.16151 -0.49981 0.13384 0.35293 0.034782 0.513 -0.39248 0.39123 0.28351 0.62023 0.6796 0.83006 0.48423 0.70625 -0.25091 -0.23268 0.15136 -0.35641 -0.45169 -0.071751 0.61275 0.12885 -0.62407 0.38831 -0.38358 -0.40596 0.022178 0.44976 -0.11437 -0.43881 -0.72876 -0.76978 -0.42424 0.26061 -0.83878 0.7724 -0.050794 0.0083812 0.18012 -0.77772 -0.22227 0.79349 0.3748 -0.96256 -0.97854 -0.92611 -0.65978 0.080195 -0.25126 1.1857 -0.72262 1.0632 0.50418 0.07075 -0.25384 -0.57426 -0.19791 -0.68991 0.061501 -0.17089 0.18609 -0.78265\n",
            "Skipping line: to\u00a0name@domain.com 0.33865 0.12698 -0.16885 0.55476 0.48296 0.45018 0.0094233 -0.36575 -0.87561 -0.35802 0.2379 0.31284 -0.081367 0.061482 0.81921 0.77488 0.68518 -0.48005 -0.012098 0.53366 0.038321 0.26857 0.56736 0.20427 0.2847 0.68113 -0.26921 0.10099 -0.33252 -0.22999 0.66003 0.21833 -0.086523 -0.30044 -0.42253 0.47525 -0.2165 0.3268 0.63515 -0.15657 -0.25835 -0.11663 -0.41092 -0.73779 -0.0015122 0.14481 0.13287 0.26097 0.58175 0.29285 0.27168 -0.0058512 0.27731 -0.40565 0.05047 0.059203 -0.39081 -0.098029 -0.13969 0.42714 -0.20103 -0.019703 1.1957 0.36278 -0.6468 0.096856 -0.54383 0.29666 -0.0098302 0.5042 -0.3419 -0.25067 0.72375 0.81957 0.57959 -0.28619 0.91511 -0.49127 0.42129 0.11429 -0.32411 -0.092257 -1.0342 -0.25774 -0.19044 -0.34201 1.2339 -0.65392 0.83096 -0.3133 -0.10256 -0.075641 0.88435 -0.1656 0.041536 0.23504 -0.59116 -0.12573 0.48002 0.23561 -0.16167 -0.1404 -0.45722 -0.13186 0.9173 -0.78831 -0.027372 0.036396 -0.49337 -0.2556 -0.15139 -0.53403 0.90196 0.60318 0.43602 -0.075461 -0.09196 0.14248 0.14335 0.67287 0.52725 -0.21312 -0.31636 0.63391 0.33626 -0.037607 0.60729 0.46028 0.0010946 0.22783 0.38501 -0.85127 -0.76092 0.20159 -0.13641 -0.15588 0.060973 0.13442 -0.25715 0.054989 0.82028 0.32086 -0.94738 -0.55948 -0.072619 -0.31286 0.53855 -0.27876 -0.45292 1.0417 0.59105 0.10445 -0.0042194 -0.11733 0.38836 0.43267 0.43228 -0.3419 0.091655 0.47851 0.62238 0.091656 -0.11656 -0.6118 0.076486 -0.27552 -0.33407 0.30166 -0.089347 -0.4676 0.14198 0.2924 0.69944 -0.53839 0.14589 0.36273 -0.18743 -0.32278 -0.46289 -0.0079593 0.025591 0.47468 0.79237 -0.72242 0.92688 0.49471 -0.39816 -0.099986 0.15334 -0.050664 0.74726 -0.047998 0.43063 -0.1613 -0.020673 0.98916 -0.27938 0.10797 0.55202 0.37686 0.17607 -0.27058 -0.1362 -0.024761 0.63829 -0.0054866 0.12085 0.56029 0.28192 0.29992 0.23869 0.070296 0.34272 -0.11676 -0.24377 0.90668 0.1251 -0.12149 -0.057172 0.36016 0.057005 -0.33869 0.14762 0.91449 -0.20497 -0.64492 -0.47259 -0.43348 -0.47966 -0.84446 -0.35207 0.0050338 -0.33834 0.15072 -0.9816 0.13519 -0.33266 0.11602 0.032961 -0.58063 0.039186 -0.34515 -0.088268 -0.49642 0.56537 -0.13342 0.29861 0.4289 0.72562 -0.61233 -0.41913 0.19969 -0.64272 -0.62544 0.58792 -0.33489 -0.066748 -0.42025 -0.25105 -0.52392 -0.17985 -0.059297 -0.28532 -0.76441 0.41743 0.83472 0.46515 0.3741 0.35699 -0.11646 0.10322 -0.16215 -0.30452 -0.0082364 -0.49549 0.47146 -0.054368 -0.18286 0.083092 -0.2659 0.449 0.3066 0.19909 0.21744 0.77927 -0.09052 -0.28405 0.02556 -0.27124 -0.26324 0.45414 0.19379 -0.21259 -0.38467 0.18494 -0.26692 -0.18104 0.051336 -0.25989 -0.15024\n",
            "Skipping line: .\u00a0. 0.035974 -0.024421 0.71402 -0.61127 0.012771 -0.11201 0.16847 -0.14069 -0.053491 -0.87539 -0.13959 0.29731 0.072308 -0.084514 -0.1879 0.12358 0.37639 -0.39238 -0.01111 -0.04924 0.63649 0.058814 0.19076 -0.20828 -0.11036 0.14934 0.24667 -0.39438 0.22853 -0.11201 0.33539 -0.32929 -0.049727 -0.090764 0.29095 0.27504 0.22802 -0.15616 0.37302 0.3752 -0.3677 0.1518 -0.27551 -0.63281 -0.31298 -0.22441 -0.15435 -0.64802 0.28404 0.12356 0.0034255 0.03094 0.35345 -0.46781 0.59203 -0.17966 0.27702 -0.46738 0.19438 0.21939 -0.36743 -0.084781 0.03253 -0.51323 -0.55466 0.49585 0.066985 0.47906 -0.25118 0.011123 0.15605 -1.0761 0.60875 -0.15764 0.066122 0.12779 -0.089209 0.4311 0.045732 -0.29364 -0.19994 -0.065952 0.26236 0.34039 -0.4956 -0.41187 0.055566 -0.69902 -0.057696 0.76519 0.2018 -0.34497 -0.22707 0.34316 -0.16098 0.42469 0.0080257 -0.33017 -0.43485 0.23581 -0.71085 0.27985 -0.31261 -0.012817 0.48305 -0.75151 -0.02347 -0.39653 -0.86857 0.2877 0.26678 0.22291 -0.1736 -0.12782 0.35032 0.27365 0.28287 0.093409 -0.18104 -0.088499 0.1189 0.026563 -0.027942 0.17254 -0.032427 -0.10745 -0.30334 -0.096047 -0.45369 -0.12113 0.06388 -0.31841 0.0059388 0.17693 -0.21071 -0.6171 0.0018674 0.27296 0.18762 -0.060409 0.5949 0.13994 -0.25773 0.20023 0.4918 0.010659 0.046456 0.4339 -0.10386 0.021517 -0.42845 -0.25458 -0.1307 0.28307 -0.27127 0.080445 -0.31285 -0.12807 0.71382 -0.086474 -0.35553 0.65281 -0.33706 0.38617 -0.12551 0.0056478 0.10091 -0.3638 -0.033486 0.32146 -0.28719 -0.24936 0.65761 0.24376 -0.068703 0.35459 0.080304 -0.30996 -0.20199 0.31482 0.15092 -0.25125 0.76149 -0.33679 -0.079472 -0.04 -0.024693 -0.55248 0.38834 0.14696 0.50003 -0.1377 0.20994 -0.53022 -0.23712 0.24392 0.29524 -0.20951 0.11347 0.16736 -0.057263 0.20945 -0.49785 0.22321 -0.24234 -0.076378 0.42953 0.71138 0.12599 -0.1331 -0.13823 -0.22315 -0.17269 0.43008 -0.34042 -0.23127 0.66599 0.15312 0.47323 -0.28108 0.097872 -0.33014 -0.068678 0.57197 0.099838 -0.6237 -0.22572 -0.59751 -0.30157 -0.1239 0.0057373 -0.058747 -0.030736 -0.10812 0.17601 0.26234 0.15636 -0.19436 -0.097775 0.15462 -0.083865 -0.15106 0.27862 0.28175 -0.27084 0.029867 0.082898 0.020298 0.35015 -0.027691 -0.010642 0.21173 0.090988 0.59747 0.12784 -0.31951 0.26881 -0.41771 -0.2073 -0.077332 -0.32069 -0.020763 -0.24735 -0.23254 -0.005691 -0.088722 -0.13886 -0.16886 -0.13215 -0.17242 -0.1223 -0.24484 0.1374 0.24458 -0.1688 0.64932 0.051973 -0.30896 0.1567 0.351 0.081668 -0.60955 -0.54647 -0.61302 -0.48092 0.22664 -0.27639 0.12523 -0.1358 0.3322 0.54997 -0.068345 0.07199 -0.11543 0.19326 -0.085861 -0.1098 0.034066 -0.072258 -0.58648\n",
            "Skipping line: .\u00a0.\u00a0.\u00a0. 0.033459 -0.085658 0.27155 -0.56132 0.60419 -0.027276 -0.093992 0.068236 -0.3961 -0.83028 0.17456 0.46373 0.13719 0.25598 -0.33885 0.18365 0.44451 -0.8193 -0.081032 -0.070653 0.11253 -0.0087314 0.45494 -0.13481 -0.19651 -0.0098954 0.34683 -0.010663 -0.10555 -0.027425 0.46831 -0.29624 0.0027633 -0.18621 0.32282 0.20276 0.5976 -0.15331 0.52121 0.59813 -0.28581 -0.23798 -0.078883 -0.64561 0.03057 0.28304 0.15654 -0.18034 0.48116 0.39754 0.2106 -0.039421 -0.31166 0.38636 0.64125 -0.52607 0.064417 -0.23567 0.37243 -0.089502 -0.39855 -0.12211 0.37331 -0.45538 -0.40342 0.65258 0.14624 0.3247 -0.41644 0.15981 0.092788 -0.47863 0.64507 -0.013909 0.21356 0.39679 0.52347 0.16871 -0.017134 -0.57287 -0.47366 0.30996 -0.32248 -0.11949 -0.48315 -0.20478 0.45759 -1.0443 -0.58684 0.58544 -0.081284 -0.21224 -0.25302 0.90371 -0.20399 0.65895 -0.11742 -0.13352 -0.091149 0.11375 -0.18618 0.098569 -0.2067 -0.22156 -0.1557 -0.7965 -0.23144 -0.31047 -0.46223 0.26712 0.31644 -0.066401 -0.40895 0.11665 0.50156 0.47769 -0.075403 -0.482 0.033416 0.17506 -0.063225 0.19088 0.039387 0.2396 -0.35331 -0.09274 -0.33705 0.085312 0.080227 -0.020173 0.507 -0.072361 0.13175 -0.45573 0.20334 -0.056897 -0.11733 0.047485 0.016908 -0.24814 0.71598 -0.055958 0.37822 0.3392 0.17434 -0.37932 0.03775 0.019235 -0.29447 -0.18964 -0.45798 -0.16 -0.056206 0.042038 0.64842 0.47592 -0.67641 -0.57816 0.769 -0.096332 -0.064562 0.042617 -0.23167 0.51994 0.10079 0.15415 -0.14515 -0.21889 0.23307 -0.043176 -0.41485 0.1427 0.84153 -0.078821 0.060877 0.12004 -0.26696 -0.53933 -0.36731 -0.37563 -0.35086 -0.24055 0.49705 -0.34699 0.021831 0.53459 0.2906 -0.065185 -0.19822 0.33217 0.28218 0.34624 0.36723 -0.66741 0.082508 -0.10312 0.27671 -0.55848 0.67853 0.040049 -0.0057962 0.86106 -0.40337 -0.45425 -0.48904 -0.11567 0.71084 -0.00071633 -0.075139 0.29584 -0.38594 0.17853 0.033168 0.19186 -0.10717 -0.51614 0.19278 -0.43339 0.51961 0.10363 0.28009 -0.41613 0.34869 0.052315 -0.026509 -0.64101 -0.047879 -0.42739 -0.018592 0.18577 -0.16994 -0.18489 -0.076386 -0.28981 0.26335 0.21274 0.11926 -0.32697 0.22216 -0.24976 0.36953 0.24742 0.37245 0.37218 -0.18512 -0.23093 0.035941 0.2418 0.058993 0.16378 0.43893 0.19057 -0.15457 0.17481 -0.30859 -0.1335 0.21542 -0.44562 0.48477 0.28185 -0.063253 0.053603 -0.05103 -0.29387 0.17704 0.64601 -0.11012 -0.23288 -0.60988 -0.33054 -0.25336 -0.042175 -0.43281 -0.061033 -0.097728 0.42459 0.051169 -0.37042 0.053402 0.18471 0.37567 -0.66422 -0.2099 -0.92126 -0.098194 0.034713 0.19768 0.35687 -0.03349 0.72838 0.1016 -0.42271 -0.19997 -0.21866 -0.2066 -0.59319 -0.052248 -0.032041 0.068443 -0.92476\n",
            "Skipping line: email\u00a0name@domain.com 0.33529 0.32949 0.2646 0.64219 0.70701 -0.074487 -0.066128 -0.30804 -0.71712 -0.65856 0.21812 0.20661 0.079468 -0.37277 0.69841 0.40459 0.10838 -0.84972 0.084111 -0.099199 -0.15883 -0.15245 0.10664 0.2866 0.058125 0.10893 -0.042369 -0.10781 -0.10683 -0.61831 0.71169 -0.4143 0.46387 0.11437 0.11797 0.78043 0.15162 0.050014 0.27046 -0.15689 -0.35901 -0.22091 0.2174 -0.53654 -0.26149 0.071581 0.28225 0.61944 0.56737 0.054449 0.63952 -0.25328 0.34196 -0.39231 0.14372 -0.3836 -0.43978 -0.13577 -0.38214 0.58241 -0.49185 0.020045 0.025588 0.13079 -0.43368 0.39428 -0.48619 0.53205 0.231 0.026263 0.20574 -0.5681 0.23058 -0.20142 0.23862 0.34402 0.53577 -0.36288 0.24647 -0.15684 -0.49296 0.063751 -0.35141 -0.10532 0.0045396 -0.047133 0.9011 -0.60606 0.073045 -0.4576 0.052548 -0.4998 0.55807 -0.21151 -0.60018 1.0234 -0.80815 0.15263 0.47984 0.09278 0.35607 -0.28802 -0.14008 -0.12359 0.48392 -0.54092 0.26861 0.18011 -0.5053 0.35732 0.027227 -0.63815 0.65042 0.60165 0.71557 -0.58427 0.0075552 0.071666 0.31442 0.5006 0.14557 0.057359 0.23515 0.09242 0.40623 0.12675 0.40275 0.11623 0.13235 0.048337 0.40902 -0.46324 -0.36735 0.10782 -0.1503 0.20129 0.1842 -0.20326 0.025293 -0.60814 0.94863 0.19966 -1.0322 -0.33815 0.42628 -0.18858 0.70273 -0.10762 -0.57743 0.14396 0.57879 0.55755 -0.10448 -0.08187 -0.0042659 0.41304 0.61282 -0.27785 0.19742 0.62788 0.44321 -0.0018459 0.076699 -0.37352 0.64692 -0.21255 0.12151 0.14871 -0.35841 -0.14698 -0.49289 0.36204 0.66804 -0.49378 0.21669 0.021931 -0.33109 -0.00088334 -0.48386 0.64701 0.10726 0.26803 0.54351 -0.19216 0.82757 0.092724 0.20233 -0.057761 0.14353 0.35351 0.053238 -0.34339 0.50812 0.027739 -0.17723 0.91226 0.24673 -0.16193 0.48266 0.17719 0.37488 -0.13047 -0.071644 -0.050737 0.36963 0.54698 -0.027903 0.010671 0.059521 0.37979 -0.0033416 0.43249 0.092561 -0.10606 0.13377 0.72729 -0.0078999 0.30048 0.12956 0.82322 0.71878 -0.6804 0.24552 0.49127 -0.62899 -0.757 -0.20259 -0.43667 -0.38061 -0.97217 -0.059481 -0.1246 0.23146 0.11842 -0.32492 0.41028 -0.43662 -0.16969 0.29064 0.053265 0.012703 -0.050542 -0.15929 -0.14017 0.46322 -0.25027 0.26577 0.65584 0.32285 -0.3679 0.1908 -0.30116 -0.61803 -0.17922 0.21711 -0.26571 -0.26099 -0.44266 -0.1252 -0.43041 -0.011423 0.30861 0.11921 -0.24908 0.58099 0.73546 -0.06392 0.18363 0.2339 0.042055 0.040574 0.47928 0.25323 0.019253 -0.2219 0.36961 0.021001 -0.0050943 -0.18382 -0.7743 0.24078 0.31495 0.35245 -0.4387 0.21064 0.41219 0.48009 0.074987 -0.11304 0.37291 0.43416 0.15984 0.30369 -0.45189 0.040446 -0.33307 -0.41444 -0.13447 0.09813 0.30967\n",
            "Skipping line: or\u00a0name@domain.com 0.48374 0.49669 -0.25089 0.90389 0.60307 0.11141 -0.021157 0.10037 -0.15173 -1.0246 0.27841 -0.062042 -0.061043 -0.47401 0.60886 0.29636 -0.077564 -0.81931 -0.023403 0.30916 0.029043 -0.34372 -0.20211 0.17291 -0.49248 0.19652 0.30329 -0.023517 0.062654 -0.56825 0.20126 -0.66776 0.064812 -0.024986 -0.081007 0.38214 -0.0035715 0.40572 0.090373 -0.023844 -0.14541 0.049812 0.054826 -0.53691 -0.14741 -0.097398 0.028146 -0.011163 0.71235 0.36371 0.66517 -0.043227 0.34479 -0.4558 -0.012266 -0.4921 -0.98134 -0.063421 -0.11077 0.46109 -0.82556 -0.25896 0.48532 0.31552 -0.57685 0.63555 -0.40461 0.41238 0.0662 0.17397 -0.1019 -0.65447 0.20334 0.053966 -0.15827 0.27588 0.43867 -0.081736 0.47221 -0.45256 -0.43325 0.14461 -0.14118 0.20053 -0.3692 0.018312 0.8577 -0.61049 -0.15427 -0.4262 0.34196 -0.36692 0.51229 -0.14486 -0.73069 1.002 -1.0086 -0.18131 0.10296 0.10914 0.34994 -0.094706 -0.22878 -0.24424 0.43829 0.063811 0.22713 -0.023387 -0.38057 0.20119 -0.027965 0.027987 0.33908 0.63356 0.64577 -0.73545 -0.064695 -0.072888 0.49183 0.26422 0.2987 0.17597 0.22247 0.082598 0.13588 0.15017 0.25407 0.27976 0.49918 0.4207 0.2508 -0.31198 -0.2434 0.079441 -0.32652 0.34065 0.090272 0.074694 0.030794 -0.69689 1.2249 0.24339 -1.0911 0.17123 0.12347 -0.22785 0.36976 -0.55589 -0.87727 0.33553 0.18196 0.3075 0.054316 -0.14845 -0.30466 0.33621 0.6362 -0.38048 0.15098 0.2354 0.81349 -0.30932 -0.13298 0.11503 0.82272 0.35549 0.059715 0.23187 -0.15168 0.17065 -0.15352 0.43389 0.64511 -0.033239 0.26057 -0.081056 -0.43354 -0.067527 -0.27017 0.3825 0.078534 0.19403 0.92968 0.061512 0.84848 0.29075 0.48908 0.043084 0.19724 0.81964 0.11703 -0.029999 0.85199 0.076113 -0.56209 0.86088 0.50691 0.10746 0.45046 -0.0016683 0.51397 0.14412 -0.45557 0.093795 -0.023058 0.20497 -0.23176 0.19408 0.096653 0.52317 -0.1884 0.04104 0.046953 -0.11417 0.035494 0.5119 0.23042 0.19461 0.36794 0.63884 0.52288 -0.33988 0.60725 0.35293 -0.73891 -0.3898 -0.23657 -0.5773 -0.24254 -0.98802 0.27399 -0.26853 0.23769 0.21178 -0.08188 0.29739 -0.50025 -0.26005 0.43288 -0.22978 0.20331 -0.36308 -0.022572 0.042341 0.43218 -0.016921 0.13097 1.0983 0.29603 -0.29021 0.020473 -0.31833 -0.58473 -0.098 0.049079 -0.52673 0.20277 -0.38167 -0.42331 -0.33024 -0.6353 0.11201 -0.0072223 -0.098756 0.084024 0.38257 -0.17998 -0.10995 -0.17451 0.57884 0.051173 0.33876 0.17066 -0.0044339 -0.48094 0.45427 0.21647 -0.33465 -0.30874 -0.84948 0.23015 0.26173 0.48439 -0.37949 0.16163 0.42102 -0.20904 -0.028854 -0.25135 0.6825 0.50248 -0.045756 0.41991 -0.22363 0.43526 -0.27397 -0.17557 -0.21913 -0.10409 0.64972\n",
            "Skipping line: contact\u00a0name@domain.com 0.016426 0.13728 0.18781 0.75784 0.44012 0.096794 0.060987 0.31293 -0.15884 -1.2367 0.43769 0.10465 0.048858 -0.23182 0.71125 0.022376 0.63524 -1.4974 0.12243 -0.07386 -0.021514 -0.37652 0.17503 -0.011225 -0.12668 -0.0090601 0.38418 0.11132 0.15851 -0.47498 0.33619 -0.48833 0.23423 0.13258 0.29362 0.13526 -0.05115 -0.0055236 0.27734 -0.23565 0.19571 -0.29095 0.062419 -0.47502 -0.71402 -0.36384 0.53562 0.40136 0.30963 0.16238 -0.11662 -0.16201 0.30672 0.21663 0.086839 -0.38895 -0.19644 -0.52311 -0.33153 0.27012 -0.89654 -0.15193 0.12447 -0.19112 -0.494 -0.011873 -0.41412 0.52585 0.27316 -0.047525 -0.1178 -0.3371 0.61151 -0.012169 0.36935 0.32679 -0.098269 0.038729 0.003551 -0.51871 -0.48189 -0.079238 -0.34291 -0.44045 -0.24479 0.05593 0.83227 -0.55939 -0.29242 -0.19718 0.17693 -0.12205 0.55837 -0.28505 -0.64676 0.57716 -1.4398 0.066288 -0.086048 0.381 -0.25805 -0.11941 -0.25664 -0.057845 1.0033 -1.0863 0.14343 0.17181 -0.81313 0.19286 0.12922 -0.20835 0.98495 0.58797 0.33635 -0.68359 0.5062 0.10678 0.57212 0.59786 0.053268 0.028426 0.42805 0.23711 0.66162 0.2584 0.32478 -0.041477 0.35121 0.19489 0.0080291 -0.36795 -0.38805 0.17224 -0.36105 0.36373 0.011708 -0.056135 0.26371 -0.59047 1.1774 0.36488 -0.66474 -0.5875 0.38414 -0.055643 0.39133 -0.34066 -0.47462 0.58672 0.46918 0.6621 0.085039 -0.26208 -0.27596 0.51976 0.78402 -0.21788 0.41615 0.057734 0.48454 0.032466 -0.57003 -0.24011 0.64941 0.1786 0.16456 0.22892 -0.34876 -0.21609 -0.40112 0.69337 0.85653 -0.14865 0.55925 0.28205 -0.48622 0.041444 -0.52311 0.89091 -0.14692 0.59723 0.45691 -0.34411 0.71453 -0.028327 0.066396 -0.28523 -0.066252 0.52379 0.26523 -0.07658 0.41631 -0.56412 -0.11156 0.55618 0.58285 0.19863 0.28857 0.26299 0.9523 0.15152 -0.021361 0.024232 0.29266 0.47276 -0.061013 -0.34556 -0.065448 0.51832 -0.01764 -0.065723 0.035893 -0.57871 -0.36286 0.53646 -0.30941 0.28428 0.035597 0.74796 0.71145 -0.51704 -0.059401 0.29316 -0.33866 -0.47136 -0.26915 -0.48318 -0.73448 -0.82325 0.18229 -0.19008 -0.17385 0.34227 -0.016458 0.15097 -0.77959 -0.72615 0.38242 -0.25839 0.067803 -0.22942 -0.70517 -0.31424 0.57725 -0.18576 0.3782 0.72404 0.56874 -0.16028 0.13383 -0.54238 -0.47211 0.30398 -0.20181 -0.35449 0.40175 -0.58787 0.2751 -0.0001217 0.1691 0.33633 -0.1821 -0.33645 0.3404 0.60535 -0.17278 0.31449 0.11914 -0.22371 0.043944 0.45005 0.89327 -0.18597 -0.77009 0.79949 0.378 0.026014 -0.39198 -0.80185 0.24173 0.22765 0.75733 -0.37899 0.30622 0.56785 0.43714 0.18369 -0.027663 0.099504 0.33348 0.088492 0.080002 -0.75372 0.26302 -0.14392 -0.37882 0.31386 -0.084045 0.70962\n",
            "Skipping line: Email\u00a0name@domain.com 0.37344 0.024573 -0.12583 0.36009 0.25605 0.07326 0.3292 -0.0037022 -0.10145 -1.8061 0.17271 0.40612 0.14999 0.077813 0.87133 0.37621 0.2232 -1.5266 0.38978 0.072969 0.4398 0.25206 0.22285 0.35195 -0.15422 -0.13137 -0.033746 -0.16894 0.14939 -0.85071 0.8303 -0.22077 0.32756 -0.46347 0.15704 0.15775 -0.075318 -0.024818 -0.045062 0.076934 0.048395 -0.72626 0.46597 -0.26966 -0.58763 0.23509 0.41876 0.38672 0.38338 0.32791 0.1036 0.31148 0.36017 -0.17754 -0.12346 -0.28095 -0.12792 -0.4066 0.14674 0.66217 -0.48934 0.33352 -0.08094 -0.081471 -0.56518 0.60675 -0.071591 0.55095 0.37616 -0.11914 -0.22842 -0.72311 -0.30394 0.06935 0.11861 0.18844 0.40996 -0.30491 -0.073846 -0.24488 -0.85284 -0.0016392 0.058739 -0.11254 0.21054 -0.12957 0.60001 -0.57975 0.098064 0.25146 0.38879 -0.59721 0.74942 0.13982 -0.64333 0.8068 -0.8529 -0.097729 0.27562 0.024347 0.32667 0.03216 0.28938 0.042973 0.66781 -0.62474 0.050046 0.11425 -0.1425 0.096074 0.45494 -0.089113 0.52999 0.30064 0.51882 0.062207 0.4262 0.30348 0.58217 0.4461 -0.083093 0.23491 0.32839 -0.20729 -0.18414 0.25996 0.75131 0.1707 -0.22728 -0.02473 0.44001 -0.67008 -0.1713 0.045325 -0.19009 -0.053806 0.49594 -0.15684 -0.25893 -0.29879 0.68854 0.052915 -0.55306 -0.12448 0.16334 0.39501 0.45141 -0.051286 -0.23803 0.31448 0.098186 0.5036 0.22789 -0.22292 -0.30905 -0.27327 0.4907 -0.31093 0.034955 0.53066 0.8932 0.19769 0.02168 -0.044989 0.80471 0.13248 -0.004733 0.53816 0.0021613 -0.34276 -0.17542 0.6035 0.51525 -0.2402 0.31781 0.028639 -0.4713 -0.029991 -0.58394 0.31659 0.075969 0.37688 0.47865 -0.62708 0.47169 0.74507 0.15765 -0.17955 0.095543 0.43644 0.056638 -0.46781 0.3761 0.21661 -0.18378 0.34377 0.40629 0.47489 1.0477 0.11735 0.22259 0.38282 0.098808 -0.088356 0.089153 0.099398 -0.090611 -0.54011 0.15283 0.06929 -0.27999 -0.076277 0.18012 -0.53787 -0.34725 0.71109 -0.28219 0.20893 0.099495 0.59917 0.48383 -0.13997 0.5227 -0.19693 -0.24871 -0.63357 -0.09565 -0.69101 -0.25347 -0.85556 -0.15462 0.0071559 0.038633 0.67575 -0.20821 1.0249 -0.50038 0.012226 0.57338 -0.32834 0.78573 -0.087241 -0.7386 -0.24992 0.43287 0.24101 0.73668 0.64582 0.26793 -0.2846 -0.34104 -0.0062504 -0.64824 0.58085 -0.20472 -0.41832 -0.023617 -0.44203 -0.017908 0.14106 -0.33705 -0.65206 0.45318 -0.53319 0.57341 0.5229 0.024291 -0.59827 -0.026417 0.10453 0.12568 0.80541 0.77114 0.069694 -0.62583 0.3634 0.11365 0.076932 0.21719 -0.50624 0.1619 0.66214 0.12204 -0.20929 -0.030636 0.64906 0.1983 0.12721 -0.67107 0.6206 0.71175 -0.45709 -0.35856 -0.51486 0.20406 0.16845 -0.24387 0.024908 0.094458 0.18698\n",
            "Skipping line: on\u00a0name@domain.com 0.037295 -0.15381 -0.045189 1.0566 0.42898 0.24093 0.34305 -0.090393 -0.79877 -1.2107 0.31958 0.46744 -0.20072 -0.61936 0.69963 0.70189 0.58516 -1.6244 0.45742 -0.10967 -0.068128 0.11185 0.1758 0.36282 0.207 0.085671 -0.024428 0.064891 0.27972 -0.62379 0.11502 -0.48324 0.32067 0.064549 0.13098 0.42373 -0.2884 0.062554 0.28625 -0.40777 -0.15946 -0.3914 -0.18405 -0.69952 -0.41679 -0.27461 0.5992 0.031153 0.57437 0.19658 0.3127 -0.078999 0.03098 -0.53815 0.091315 -0.38143 -0.75908 -0.12581 -0.20777 0.24508 -0.51285 0.020266 0.16943 -0.21203 -0.14944 0.46839 -0.40152 0.24182 0.059784 -0.10328 -0.23935 -0.15119 -0.052004 -0.078992 -0.35178 -0.21634 0.59896 -0.47535 -0.11839 -0.21983 -0.61098 -0.31832 -0.55874 0.052747 0.039198 -0.56885 1.0475 -0.22668 -0.53216 -0.12656 0.29789 -0.12112 0.59625 0.076639 -1.0092 0.66201 -0.51496 -0.076104 0.082936 -0.0094538 0.63184 0.20894 0.15745 0.074381 0.31779 -0.68213 -0.18466 -0.074668 -0.48459 0.06025 0.43421 -0.26723 0.7676 0.5288 0.27123 -0.82053 0.26875 0.25783 0.4725 0.94076 -0.27055 0.11796 0.16388 0.16271 0.71576 -0.84291 0.27383 0.12842 -0.13808 -0.57815 -0.16535 -0.45732 -0.59394 -0.063703 0.1311 0.33161 0.27681 -0.049656 0.035738 -0.29478 1.111 0.086364 -0.63429 -0.28759 0.16863 0.088104 0.33693 -0.24049 -0.40589 0.63117 0.2691 -0.0059601 0.077498 0.0048893 -0.42893 0.19667 0.18104 -0.033527 0.41909 0.56974 0.22464 0.44984 -0.3836 -0.056989 0.70036 -0.062551 -0.076638 0.41616 -0.43017 0.051679 -0.45284 0.59514 0.74749 -0.49283 0.30089 0.020652 -0.28359 -0.15402 -0.60682 -0.044729 0.27258 0.17982 0.69056 0.26935 0.88267 -0.25968 0.018912 -0.06569 0.2459 0.57953 0.38363 -0.38261 0.89573 -0.030606 -0.19293 1.0351 0.47315 -0.029376 0.29365 0.53393 0.23375 0.46295 -0.3838 0.33758 0.40473 0.5511 0.0051471 0.098574 -0.0016179 0.055963 -0.26139 0.075229 -0.16292 0.24037 -0.07256 0.22245 -0.0040634 0.12477 0.16262 0.61958 0.10218 -0.30888 0.41935 0.23097 -0.78383 -0.42864 -0.43192 -0.52814 0.13438 -0.83851 -0.41554 -0.24818 -0.20883 0.75631 -0.25623 0.32551 -0.6399 0.26467 0.18173 -0.32961 0.028943 0.44702 -0.31151 0.34936 0.61233 -0.34992 0.68184 0.64509 -0.098248 -0.45894 0.20604 -0.334 -0.40975 0.17371 -0.15081 -0.93562 0.091008 -0.41959 0.026537 -0.37542 -0.39248 -0.01727 0.1491 -0.42839 1.2822 0.86974 0.4999 -0.09103 -0.44413 0.15755 0.34085 0.51527 0.41727 0.2675 -0.37372 0.35689 0.241 -0.65587 -0.11725 -0.6031 0.1791 0.32321 0.27526 -0.18872 0.12421 0.077509 0.6748 -0.053329 0.071209 0.47614 0.35372 -0.26264 0.067594 -0.063179 0.2563 0.059929 0.30376 0.16772 -0.067342 0.30749\n",
            "Skipping line: At\u00a0Killerseats.com -0.13854 -0.01706 -0.13651 0.1237 0.15633 -0.16556 0.29374 -0.064174 0.3209 -0.091516 -0.017139 -0.061828 -0.17736 -0.037355 0.0015256 -0.13485 -0.1714 -0.040029 0.12924 0.06956 0.02036 -0.011202 0.0039595 -0.1076 -0.014068 0.091797 0.0064314 -0.14604 0.0039716 -0.045866 -0.16544 -0.13641 0.099001 0.10089 0.018211 0.045112 0.18615 -0.018217 -0.085986 -0.13309 -0.0047582 -0.076491 0.23942 0.061977 -0.11535 -0.02209 0.1325 -0.0012518 0.10991 -0.18539 -0.12344 -0.06419 0.0020279 0.08325 0.23703 -0.20077 0.0068709 -0.24464 -0.019755 -0.11095 0.0046046 -0.20673 -0.049618 -0.067157 0.02305 0.09159 -0.11349 0.030432 -0.034791 0.1984 -0.056183 -0.10847 0.0062699 0.047591 0.0012842 0.039199 -0.086015 0.12928 0.27708 -0.11308 -0.030623 0.10101 -0.26294 0.021336 -0.030286 0.094648 -0.26473 0.20893 0.011739 -0.27749 -0.26562 0.015384 -0.066983 0.091436 -0.01803 -0.10952 0.033702 0.19421 0.018353 0.067715 -0.14521 0.076887 0.20074 0.049053 0.031508 0.18307 0.12594 -0.012344 -0.00075399 0.20056 0.18769 -0.13718 0.23926 0.15341 0.068396 0.10398 0.12115 0.049698 -0.0432 0.11788 0.063636 -0.051266 0.00095202 -0.063047 -0.11646 -0.0499 -0.010252 0.037608 0.027558 -0.047844 0.18169 -0.026459 -0.08777 0.0095411 0.17127 -0.10256 -0.17665 -0.0016558 -0.29702 -0.14523 -0.060911 -0.12564 0.27857 0.016822 0.051903 -0.21652 -0.15691 0.033816 -0.045045 -0.16395 0.12095 -0.036018 -0.12004 0.16239 0.17236 0.0043345 -0.09299 0.12625 -0.16393 -0.16347 0.13903 -0.0024316 -0.059626 0.094548 -0.16164 -0.025202 0.09998 -0.2755 -0.028732 -0.073529 -0.25114 -0.057089 -0.14148 0.011597 -0.25583 -0.040875 -0.0020845 0.24255 0.094112 0.28931 0.20464 0.15538 -0.27549 0.046415 -0.018823 0.036006 -0.1589 -0.1442 -0.11284 0.19729 -0.11681 -0.0027812 0.11 -0.069434 0.16456 -0.19392 0.03563 -4.8304e-06 0.21579 0.096814 0.17293 -0.30057 0.087337 -0.088238 -0.13899 0.16369 -0.061456 0.10775 0.23821 0.08518 0.076787 -0.091381 -0.12392 -0.12457 0.20588 -0.21204 0.064002 0.22108 -0.34458 -0.029845 -0.086207 -0.070035 0.015213 0.0044742 0.054369 0.16562 -0.071822 -0.0196 -0.18231 0.0070107 0.10313 -0.057425 0.09921 -0.17666 0.30342 -0.09457 0.087363 -0.10153 -0.15785 0.074526 -0.055874 -0.24675 -0.059254 0.093743 0.10697 0.11725 0.010904 -0.070051 -0.094899 0.12954 -0.037657 -0.11976 -0.25947 0.10308 0.12485 -0.06518 -0.018488 -0.045376 0.12019 -0.020251 -0.046336 0.056461 -0.041232 -0.0052311 0.02632 0.0015934 0.11822 0.20279 0.011894 0.32837 -0.025418 0.050482 -0.37133 -0.08665 -0.0078504 -0.031485 -0.22887 0.046758 0.10404 -0.086826 -0.31606 0.14347 0.081695 0.0028145 0.13973 0.10326 0.064327 0.06976 -0.03511 0.038678 -0.11595 0.26615 -0.22468 0.26911 0.019649 -0.13941 0.024992 -0.2906 0.32547 0.12586\n",
            "Skipping line: by\u00a0name@domain.com 0.6882 -0.36436 0.62079 1.1482 -0.055475 -0.37936 0.0064471 -0.33046 -0.43406 -1.3468 0.70312 -0.41314 -0.65868 0.64324 0.13018 0.65846 0.86269 -0.93108 0.3476 0.73912 -0.51405 -0.15113 0.27331 0.51396 -0.74688 0.87989 -0.11887 0.3641 0.37838 0.36177 -0.45182 0.16173 -0.36353 -0.55643 -1.1186 0.70117 -0.48075 0.074095 0.43022 0.4625 0.011133 0.030287 -0.73342 -0.772 0.31058 0.022106 -0.16845 -0.70695 -0.16243 -0.15454 -0.12034 0.018702 0.51626 -0.17255 0.37335 -0.059377 0.013126 -0.30727 0.1581 0.74527 -0.7927 -0.34603 -0.01438 -1.055 -0.95074 -0.81794 0.27925 -0.35405 -0.26783 -0.30391 0.16093 -0.064806 0.69283 -1.1955 0.18414 -0.71183 0.062622 -0.62435 -0.16458 -0.74362 -0.19251 -0.1841 0.99035 -0.20552 -0.46621 0.98506 1.4113 0.024391 -0.14285 0.40063 0.10516 0.065123 -0.4613 0.27429 0.022191 0.55307 0.18442 -0.22378 -0.50433 0.046039 0.12306 -0.11203 -0.30851 -0.13275 -0.36831 -0.63785 -0.99149 -0.55833 0.17128 0.27324 -0.37803 0.4641 0.39427 0.048909 0.46002 0.072136 0.090133 -0.77511 0.31636 1.0484 0.65455 -0.54555 -0.16023 0.51455 0.46977 -0.70197 -0.032668 -0.23516 -0.053423 0.6293 -0.35302 -0.44905 -0.44235 -0.14551 -0.60106 -0.68264 0.71618 0.44723 0.17647 0.60878 1.0692 -1.0488 -0.48178 0.10825 -0.27708 0.50754 0.69316 -0.40861 0.98707 0.045813 0.6237 -0.041425 0.23719 0.50664 -0.14663 0.55561 0.60422 0.089302 -0.18124 0.51483 0.51676 -0.25888 0.64182 0.22324 0.54462 0.39816 -0.97695 0.17496 -0.35361 -0.77967 0.23945 0.21711 0.26793 -0.029782 0.75712 0.083235 0.38329 0.14695 -0.42997 -0.12402 0.078012 0.69183 0.36966 0.75105 0.91858 -0.11291 0.53013 0.35894 0.65682 -0.30278 0.074167 0.064695 0.08892 -0.04165 -0.5437 1.2794 0.21133 0.47272 0.24959 -0.77101 -0.20456 0.54224 0.44168 0.11797 -0.28053 -0.30537 0.39505 -0.46693 0.54209 0.58962 0.17908 0.15278 0.51678 1.0729 -0.21624 -0.63379 0.6209 -0.029061 0.45802 -0.70106 -0.096258 -0.13379 0.99638 0.62168 0.045304 -0.85691 0.15816 -0.77156 0.45802 -0.48487 0.10298 0.12015 -0.61097 -0.51157 -0.49761 0.34814 0.20141 0.33286 1.2239 -0.42395 -0.33754 -0.51882 -0.66346 -0.32048 0.25638 0.06427 0.22788 -0.32311 0.11933 -0.47903 0.049264 -0.35126 -0.79376 -0.073158 0.25249 -0.92067 -0.2175 0.4002 -0.17607 -0.49748 0.028808 -0.66234 -0.15404 -0.51768 0.023225 0.46048 0.19184 0.17417 -0.32702 0.0023848 0.36328 0.40963 0.2658 -0.016626 -0.85864 -0.77511 0.71664 -0.8535 -0.53959 -0.29513 -0.11449 0.25627 0.3023 -0.32311 0.58707 0.61075 -0.68101 -0.26994 -0.50763 0.23818 1.1097 -0.5269 -0.036155 -0.20817 0.2446 -1.0573 -0.11534 0.08409 -0.47809 -0.32664\n",
            "Skipping line: in\u00a0mylot.com -0.18148 0.47096 0.32916 0.044196 -0.93045 -0.16299 0.31996 0.39017 0.013753 -1.0117 1.4497 0.14411 0.09409 -0.034215 -0.11889 -0.24614 0.88631 -0.62527 0.1653 0.17695 -0.092857 -0.82453 -0.3729 -0.32292 -0.048221 -0.42264 0.11826 0.17933 -0.058891 0.16458 0.09932 0.4913 0.051474 -0.059584 -0.086531 0.060116 -0.24148 0.34348 0.011429 0.046588 0.33665 -0.63822 0.036459 0.65724 0.17596 -0.33444 -0.17779 -0.047878 0.29427 0.10278 0.31098 0.62907 -0.098816 -0.5123 -0.43283 0.47748 -0.13249 -1.0708 0.081945 -0.04147 -0.23569 0.3655 -0.30838 -0.76359 -0.36836 -0.22545 -1.1477 -0.12543 0.087085 0.10207 -0.24626 -0.41621 0.20584 -0.034737 -0.79378 -0.32414 0.32755 0.66629 0.058202 0.13797 0.39361 -0.41695 -0.40613 -0.076039 -0.11123 -0.17525 -0.0088467 -0.044072 -0.69197 -0.74632 0.39905 -0.047611 0.19114 0.83765 0.42194 0.4051 0.18833 0.76741 -0.50475 -0.38181 0.054343 -0.22186 -0.18301 -0.1434 -0.66303 0.54454 -0.19581 -0.37495 0.37518 -0.14736 0.12769 0.066166 -0.66023 -0.13895 0.47418 0.44716 0.14277 -1.1752 0.17875 -0.26377 -0.71254 -0.056002 1.1058 0.48956 -0.0034874 -1.2341 -0.25508 0.44549 -0.47797 0.38046 0.062388 -0.30549 0.018524 -0.73576 0.11699 -0.48136 0.68783 0.34749 0.33697 0.25123 1.6869 0.22458 -0.25622 0.29453 -0.67176 0.77372 0.17065 -0.90475 0.043384 -0.1352 -0.035265 -0.70517 -0.45938 -0.068112 -0.51148 0.47336 -0.10527 -0.4393 -0.38211 -0.27846 0.19689 0.075033 0.32149 -0.24502 0.054691 0.11946 -0.26407 -0.0074057 -0.66552 -0.20797 -0.043512 0.53229 -0.28674 0.74577 0.27161 -0.18583 0.020504 0.032247 0.093833 0.16584 0.0064513 -0.11134 -0.6277 0.66321 1.0995 0.17221 0.39757 0.22174 0.37047 -0.11836 0.046102 -0.21112 0.41396 -0.0033789 -0.41364 0.41285 0.06004 0.067526 -0.21855 -0.48179 0.16688 0.59051 0.21349 0.41751 0.065228 -0.83219 0.28468 0.027128 0.56385 0.5687 0.12164 0.26231 0.10388 -0.051454 0.16501 -0.92392 -0.10778 -0.31637 0.28046 0.080897 -0.020649 0.56557 0.56447 0.48398 -0.85527 -0.17136 0.038894 -0.28895 -0.19452 -0.17616 0.39584 0.43994 0.36613 0.012852 -0.13704 0.11059 -0.39179 -0.03542 0.084348 -0.41242 -0.59482 -0.51298 0.104 -0.083639 -0.84864 0.34791 -0.22998 0.41067 0.20366 -0.56011 -1.0281 0.66185 0.11875 1.1039 0.15888 -0.18237 -0.50039 -0.058217 -0.60943 0.35013 -0.62093 0.098393 0.50862 -0.10342 -0.14692 0.014277 0.63924 0.14633 -0.2731 0.069363 0.78212 0.17801 -0.81651 0.087242 -0.83718 0.037194 0.29325 -0.062079 0.038745 0.12474 -0.40302 0.11265 -0.82604 -0.12445 0.016351 -0.41329 -0.58576 -0.44029 -0.0035195 0.26308 0.071372 0.43961 -0.34098 -0.27518 0.20454 0.32802 -0.39246 0.11756 0.1804 -0.17405\n",
            "Skipping line: emailing\u00a0name@domain.com 0.39173 -0.39132 -0.4266 0.82429 0.42919 0.17601 0.16663 -0.011601 -0.53551 -1.255 0.35066 0.06361 -0.70235 -0.31163 0.25379 0.2463 0.21129 -1.5845 0.19661 -0.01602 -0.54956 -0.58445 0.03272 0.90671 0.33191 -0.52747 0.32609 0.53668 -0.18302 -0.49699 0.069751 -0.32102 0.034384 0.30422 -0.37326 0.73485 -0.36973 0.89839 0.30623 0.18463 -0.23602 -0.17 0.34307 0.026764 -0.1685 0.4373 0.43021 -0.17562 0.15805 0.19768 0.064745 0.048354 -0.17168 -0.24719 -0.2572 -0.59113 0.1086 -0.20647 0.19999 0.25039 -0.50867 0.013532 -0.1393 0.20503 -0.94526 0.76842 -0.25137 0.062295 -0.0779 -0.43982 0.17724 -0.5318 0.15458 -0.26637 -0.34106 -0.26619 0.39768 -0.0027818 0.17052 -0.29347 -1.3207 -0.43398 -0.22671 0.026272 0.12023 0.12684 0.92681 -0.62325 -0.086858 -0.17383 0.38999 0.31858 -0.32013 0.32169 -1.4042 0.774 -1.0086 0.076515 0.22546 -0.34501 0.64108 -0.62987 -0.33897 0.69475 0.50582 -0.79349 -0.41876 0.66703 -0.081664 -0.079977 -0.099099 -0.41525 0.9304 -0.13361 0.17219 -0.32933 -0.29329 -0.1672 0.12365 0.75392 0.49888 0.51591 0.5768 0.19417 0.49986 -0.60501 0.5596 0.16308 0.23527 0.34854 -0.4135 -0.4168 -0.34117 0.41811 -0.22984 0.045453 1.021 0.15702 -0.16201 0.34939 0.74524 0.64977 -1.0165 -0.24076 0.024844 0.44116 0.79557 -0.72798 -0.046033 0.28843 0.41386 0.38713 -0.29446 -0.1907 -0.67133 0.57932 -0.1499 -0.049878 -0.2862 1.0487 0.38315 0.5175 0.17042 -1.03 0.6565 0.075288 0.21532 0.2055 -0.10369 -0.24518 -0.79469 0.54382 0.56461 0.12539 0.23951 -0.17072 0.05467 -0.31934 -0.699 0.40972 0.00091743 0.32084 0.050919 -0.19141 0.90119 0.39184 0.97954 0.036333 0.66042 0.94628 0.74539 -0.36498 1.0807 -0.050241 0.16434 -0.014925 0.9391 0.51309 0.4418 0.42688 0.37485 0.021399 0.3364 0.061034 0.87373 0.20534 0.51232 0.44053 0.35507 0.17071 -0.46624 0.21338 -0.28831 0.14749 0.50913 0.52774 0.3225 0.18207 0.25947 0.53903 0.78083 -0.47849 -0.37548 -0.2368 -0.26527 -0.31599 -0.72522 -0.2193 0.14468 -0.51742 -0.29822 0.52975 0.19253 0.02847 -0.12754 0.31759 0.073123 0.23208 -0.32065 -0.64801 -0.49646 0.34046 -0.16553 -0.60268 0.44127 -0.53358 0.53215 0.20003 0.36597 -0.064675 0.06453 -0.18848 -0.58001 -0.13561 0.12283 -0.53895 0.12008 -0.52819 -0.15895 0.23558 -0.32919 0.15223 -0.15 -0.8083 0.25604 0.81459 0.18576 0.048995 0.48536 0.012105 0.43842 0.26448 0.52854 0.16257 -0.20932 0.089357 -0.014704 -0.15414 0.13277 -0.43273 -0.43306 0.0024893 0.15383 -0.27752 0.67561 0.68944 1.0019 0.52244 0.14167 0.51255 0.84324 0.19646 -0.055988 -0.65196 -0.023645 0.26897 -0.13204 0.34183 -1.1074 -0.21701\n",
            "Skipping line: Contact\u00a0name@domain.com 0.14933 -0.28605 0.3444 0.29015 -0.22999 0.1271 0.35722 0.35118 0.28224 -1.3679 0.44651 -0.017463 -0.12998 0.39657 0.71315 0.11213 0.70139 -1.5873 0.7609 0.072771 0.66317 -0.33039 0.31256 -0.020889 -0.47135 0.3383 0.53767 0.057899 -0.023796 -0.31841 0.23784 -0.28834 0.17782 -0.5244 0.29458 0.44297 0.53541 -0.10591 1.0599 0.18864 0.55033 -0.50063 0.022688 -0.17678 -0.70947 -0.8091 0.27532 -0.048975 0.32952 -0.26311 0.54794 0.50121 0.61235 0.039506 -0.39968 -0.84155 -0.52122 -0.6594 -0.43807 0.54237 -0.79301 0.034652 -0.12363 -0.35739 -0.46396 0.92434 0.29489 0.26151 -0.25829 -0.55722 -0.14739 -0.21945 0.46516 -0.44058 0.41805 -0.011851 0.5441 -0.21327 0.043397 -0.10785 -1.3427 -0.37442 -0.40596 0.044713 0.071265 0.22554 1.0166 -0.18678 -0.080248 -0.070555 0.38612 -0.034025 0.33291 0.16814 -0.33787 0.48429 -0.9275 -0.37455 0.28879 0.011307 -0.034424 -0.074561 0.33677 0.086636 0.58377 -0.97006 0.21602 -0.10354 -0.18782 -0.18874 -0.0075329 0.5045 0.41263 0.63932 0.46135 0.043514 0.037813 0.48608 0.45065 0.094473 -0.51254 -0.2553 0.60168 -0.22751 0.14411 -0.015375 0.26335 0.12941 0.55472 -0.35128 0.14395 -0.37141 -0.47784 0.54191 -0.30975 -0.12484 -0.10024 -0.080352 -0.79665 -0.65575 0.99039 0.092417 -1.0277 0.10834 -0.24512 -0.16528 0.034586 0.35529 -0.27859 0.28722 0.30695 0.31876 -0.4585 0.35226 -0.03028 0.047087 1.488 -0.11154 0.40292 0.092156 0.89874 0.30825 0.068088 0.072786 0.77015 0.43306 -0.089741 0.34737 -0.60596 -0.10934 0.45442 0.38238 0.90917 0.40277 0.037961 -0.71513 -0.28856 -0.21618 0.098854 0.65154 0.36816 0.12634 -0.07398 -0.81584 0.79696 0.65227 0.62773 -0.19997 0.58956 1.2545 0.16172 -0.26292 0.48653 0.12693 -0.20123 0.12498 0.43937 0.64485 0.5004 -0.11575 -0.010742 -0.094577 0.52967 0.14039 0.30212 0.58504 -0.074609 -0.35289 0.032048 0.23581 -0.47016 -0.041431 0.15496 -0.25254 -0.18551 0.34496 -0.016185 0.1799 0.76838 0.5616 -0.0043105 -0.083277 0.86002 -0.2186 -0.78777 -0.31371 -0.408 -0.30427 -0.48196 -0.69419 0.00093017 0.20153 0.46771 0.055064 0.16938 0.028213 -1.1183 0.22422 0.32095 -0.21798 0.78578 -0.4124 -0.90433 -0.23226 0.80892 0.36045 0.34569 0.7428 0.57411 0.013348 -0.24344 -0.5165 -0.12468 0.21977 -0.46848 -0.44024 0.57263 -0.39965 0.00536 0.15398 -0.57058 -0.12496 0.17647 -0.36044 -0.077987 0.2501 -0.18364 -0.106 -0.079021 -0.11581 -0.026927 0.7054 0.39769 0.30528 -0.12727 0.49794 0.52739 -0.037058 -0.61461 -0.74724 0.57124 0.0069423 -0.022656 -0.86076 0.33951 0.8744 -0.00072818 0.31948 -0.47734 0.29885 0.72535 -0.79345 -0.031487 -0.58767 0.3152 0.0075835 -0.27275 -0.017409 0.43731 -0.05677\n",
            "Skipping line: at\u00a0\u00a0name@domain.com 0.44321 -0.40005 -0.20065 1.1209 0.34041 0.086082 -0.067128 0.0022702 -0.94649 -1.4669 0.61248 0.34827 -0.20983 -0.61434 0.41102 0.57759 0.69071 -1.9301 0.75265 -0.13238 0.22003 0.28856 0.35234 0.45989 -0.21944 0.1931 -0.11664 0.14996 0.70354 -0.039238 0.55298 -0.53503 -0.3221 -0.28595 -0.1246 0.054544 -0.45937 0.1447 0.8203 -0.33182 0.10864 -0.56552 0.39898 -0.65012 -0.20285 0.11557 0.35711 -0.23958 -0.30281 0.51593 0.71883 -0.30403 0.59458 -0.3217 -0.23967 -0.2576 -0.50224 -0.36055 -0.71763 0.4981 -0.69945 -0.0072578 0.37327 -0.029839 -0.42705 0.93128 -0.046928 0.045162 -0.44879 0.16579 -0.26272 -0.35286 0.17395 -0.24436 -0.1439 -0.39857 0.25342 -0.44737 0.37618 -0.80252 -0.87776 -0.19282 -0.48746 0.065159 -0.24349 -0.77669 0.81629 -0.043888 -0.68276 -0.15709 -0.46533 -0.066009 0.063028 0.090332 -0.81297 0.88979 -0.6391 0.17351 0.3328 -0.30808 0.46158 -0.11289 -0.0261 -0.089243 0.37318 -0.73511 0.19798 -0.060219 -0.12113 -0.2146 0.62061 0.34296 0.89595 0.22495 0.16079 -0.84005 -0.6749 0.4053 0.17549 0.70242 -0.22907 0.068957 0.092359 0.034122 0.24296 -0.47643 0.27613 0.04291 -0.055271 0.35486 0.32692 -0.66761 -0.77199 -0.075454 -0.36238 0.052461 0.34678 0.070744 -0.5845 0.14793 0.81643 0.45906 -0.80253 0.18628 -0.039264 0.21197 0.45927 -0.274 -0.81792 0.73455 0.30696 0.10415 0.035539 0.012445 0.088135 0.67792 0.45663 0.2726 -0.50234 0.62198 0.38648 0.01011 -0.32851 -0.068547 0.85187 0.14533 0.28169 0.757 -0.61313 0.46784 -0.53806 0.22123 0.59195 -0.52707 0.342 -0.32091 -0.0031903 -0.063289 -0.59452 0.22196 -0.2044 0.26864 0.14327 0.077514 0.75915 0.17929 0.27785 -0.24118 0.57407 0.94331 0.61097 -0.36571 0.37771 0.30822 -0.42249 1.1221 0.67737 0.29251 0.42105 0.1204 -0.19767 0.70793 -0.19204 0.27399 0.21617 0.58442 0.020547 -0.004382 0.2435 -0.30135 -0.32904 0.2376 -0.42115 -0.2809 -0.12963 0.23212 0.28883 0.047856 0.019705 0.57196 0.56619 0.012819 0.24007 0.22556 -0.47738 -0.15644 -0.64105 -0.57604 -0.36252 -0.20212 -0.3813 0.27112 0.37976 0.20358 -0.28793 0.36938 -0.42238 0.14111 -0.19996 -0.34411 0.10342 -0.49926 -0.32485 0.42403 0.35958 -0.35653 0.27493 0.43134 0.45197 -0.17992 0.36502 -0.1422 -0.20199 -0.35023 0.16243 -1.1251 0.044546 -0.59278 0.03059 -0.44425 -0.82482 -0.21953 -0.39308 -0.84476 0.20993 0.99719 0.1983 -0.28091 -0.36525 0.52296 0.65851 0.46585 0.52257 0.14744 -0.74425 -0.12268 0.81749 -0.2461 0.018109 -1.0154 0.29857 0.32427 0.57664 -0.41371 -0.099047 0.24261 0.12247 0.31508 -0.076621 0.6407 0.25333 -0.2548 0.022574 -0.037949 0.0059522 0.068205 -0.11948 -0.44629 -0.45647 -0.15084\n",
            "Skipping line: \u2022\u00a0name@domain.com -0.13288 -0.31383 -0.032356 0.52036 -0.26985 0.43339 0.32587 -0.51581 -0.9806 -1.5879 0.73555 0.35252 -0.73296 0.6446 0.37158 -0.26666 0.21589 -0.91457 0.18992 0.81333 0.8067 0.59507 -0.5518 0.48099 0.12726 0.03783 0.24796 0.50997 0.075809 -0.75845 0.32875 -0.29168 0.057426 -0.16102 -0.68347 0.86165 0.2599 -0.56354 -0.040419 -0.28022 -0.20049 0.0064199 -0.45262 -0.10725 0.31171 0.35856 0.032082 1.203 -0.3693 -0.027401 1.1 -0.20264 0.18898 -0.45642 0.048134 0.5478 -0.14036 0.073562 1.0508 -0.12821 0.33468 0.2014 0.028187 -0.37055 0.48804 0.042724 0.030665 -0.025691 0.10987 -0.31731 -1.3087 -0.24197 -0.16837 -0.69608 0.086061 -0.17898 0.46507 0.33104 0.55842 -0.60105 -0.063918 -0.24297 0.48818 0.14094 -0.59654 0.79936 0.58047 0.24874 -0.98018 0.24882 0.25235 -0.72269 -0.2084 0.12267 0.011462 0.16495 -0.33902 -0.52693 -0.42821 0.42544 1.0436 0.31029 -0.021538 -1.0413 -0.80963 -0.098143 -0.1954 0.36169 -1.1466 -0.44647 0.35587 0.76049 0.50332 -0.52995 0.84432 0.77632 -0.18343 -0.89036 0.14031 -0.14793 -0.55824 0.16072 -0.15132 0.35349 -0.0015194 -0.1344 0.75585 1.4173 0.34559 0.84169 0.23758 -0.55682 -0.4233 0.34256 0.21639 0.23196 -0.058196 -0.34022 0.30834 0.085223 1.0005 -0.75103 -0.19934 1.1943 -0.75858 0.47305 0.2842 -0.23272 -0.31152 0.016731 0.26253 -0.11474 -0.97903 -0.0092514 0.068853 0.98873 -0.24826 0.081491 0.48154 0.83283 0.70266 0.42554 0.57001 0.98106 0.18615 0.48742 0.57736 0.55659 -0.68907 0.33097 -1.2181 -0.01721 -0.80811 -0.41657 -0.03589 0.019744 0.38763 0.75493 0.9686 -0.81071 0.28657 -0.41282 0.46566 -0.3717 1.0972 0.36665 0.90244 -0.82436 0.56426 -0.35644 0.36488 -0.47052 0.98485 -0.68457 -0.84468 -0.34603 0.71688 0.63703 -0.49426 0.11989 0.018898 1.1553 0.026494 -0.38328 -0.83349 -0.99136 -0.092262 0.39492 0.3145 -0.55455 0.12677 0.18484 0.070504 -0.16828 -0.13925 0.21572 0.29662 -0.24226 -0.19738 0.36044 -0.57074 0.2877 0.539 -0.62249 -0.61904 -0.030457 0.52519 -0.29111 -0.037765 -0.016212 0.29459 0.21433 -0.39144 -0.76351 -0.33047 0.32052 -0.066346 0.29668 0.33397 -1.3425 -0.16393 -0.15417 -0.1092 -0.48709 -0.28508 0.83707 -0.44983 0.99084 0.70429 -0.30875 -1.1595 -0.25862 0.099996 -0.20872 -0.31094 -1.0475 -0.36245 0.39012 -0.3529 0.70455 -0.33983 -0.74342 -0.12024 -0.8151 -0.40635 -0.52105 0.61012 -0.25333 -0.9383 -0.048584 -0.14662 0.48444 0.84921 1.1434 -0.34354 0.61107 -1.0268 0.41495 0.29176 -1.3315 -0.10941 0.41862 -0.14078 -0.41347 0.71217 0.5683 -0.72551 0.29458 -0.88888 0.2785 0.8716 0.42549 -1.2827 0.60796 0.10976 -0.033875 0.056314 0.86177 0.080508 0.17186\n",
            "Skipping line: at\u00a0Amazon.com -0.5275 -0.73685 0.10968 0.22214 -0.30063 -0.63201 -0.053204 -0.16241 -0.33811 -1.4072 1.1837 0.45468 -0.84209 -0.011505 0.896 0.03319 0.26241 -1.2644 0.57994 -0.23699 -0.49705 0.16729 -0.18469 0.12163 0.23774 -0.0997 -0.3207 0.56191 -0.017743 -0.32547 0.014204 -0.57266 0.42053 -0.020948 -0.29288 -0.086615 0.46141 0.28811 0.30401 -0.10123 -0.90849 0.36862 0.40785 -0.80729 0.23232 -0.14189 0.3161 0.049262 -0.33411 -0.25296 -0.019395 -0.24234 -0.59846 0.76997 0.87265 -0.24425 0.51755 -0.20878 -0.56027 -0.21439 -0.42029 0.13491 -0.028368 -0.56969 -0.1266 0.38287 -0.16767 -0.58677 -0.32237 0.15846 0.59556 -1.0552 0.79441 0.17987 -0.62691 0.38177 -0.066748 0.16996 -0.10173 0.066299 -0.55561 -0.38713 0.33772 0.33874 0.053886 -0.10488 0.56415 -0.13275 0.1262 0.29306 -0.31095 -0.17854 0.35285 0.00013075 0.51898 1.2236 -0.2643 0.037547 0.51511 0.22394 -0.25357 -1.0723 0.49631 -0.27033 0.11704 -0.67869 -0.22536 0.13763 0.1407 0.44025 0.043805 -0.11425 -0.17916 -0.10361 0.14973 0.81742 -0.0013219 -0.14155 0.19542 0.62475 -0.58424 0.31022 0.66416 -0.33286 -0.16716 0.072762 -0.49764 -0.10817 0.22071 0.14323 -0.23053 -1.1992 -0.28862 0.21024 -0.62443 0.076061 0.18231 0.087195 -0.34459 0.02946 0.67898 -0.30063 -0.174 -0.89263 -0.042857 -0.30505 0.66966 -0.013918 0.29442 0.059769 0.24655 -0.29502 -0.024101 -0.43562 -0.042243 0.48316 -0.24061 -0.55936 -0.25671 -0.16496 0.41056 -0.011025 0.3995 -0.10955 0.57864 -0.095326 -0.14995 0.10487 -0.50071 -0.022227 -0.3401 -0.12177 0.21538 0.44572 0.73496 -0.1897 0.0044095 -0.84324 -1.1227 -0.13945 -0.18635 0.047539 -0.02576 0.94764 0.22948 -0.15774 -0.25206 -0.38703 0.6866 0.20996 -0.15875 0.0058876 0.5742 -0.67418 -0.16063 -0.075739 0.42837 0.20913 0.16235 -0.65112 0.33446 0.1024 0.2308 0.26889 -0.53514 0.51401 -0.077549 0.23051 0.27226 -0.096375 0.0030776 0.58233 -0.46236 -0.25057 -0.074138 -0.32048 0.62223 0.54458 0.6217 1.1464 0.50221 -0.90171 0.31349 -1.1352 -0.19198 -0.31481 -0.12779 -0.29484 -0.22873 -0.31661 0.53138 0.24974 0.070768 -0.65404 -0.37577 0.61155 0.49496 -0.052352 0.53938 0.005889 0.21959 0.10324 -1.1018 -0.14499 0.25521 0.24265 0.31371 0.33447 -0.37324 -0.52313 0.20971 -0.13097 -0.16648 -0.049781 0.2748 -0.92401 0.75744 0.48051 -0.44793 -0.42594 0.34599 -0.53192 -0.13063 0.096985 -0.042562 0.52247 0.74385 -0.68267 -0.19708 -0.0070701 -0.02074 -0.58448 -0.065581 1.3816 -0.9249 0.033565 -0.16956 0.83749 -0.40574 -0.50804 -1.0029 -0.14006 -0.1387 -0.58868 -0.2428 -0.022855 -0.34926 0.053938 -0.60598 -0.031057 0.7892 -0.75688 0.12333 -0.094554 0.042073 -0.30965 0.24605 0.12449 -0.48643 0.23043\n",
            "Skipping line: is\u00a0name@domain.com -0.1197 0.10706 -0.10519 -0.12412 0.4096 -0.0287 0.34704 0.3549 -0.24818 -1.2408 0.080013 0.72535 0.86863 -0.044256 0.91981 0.7825 1.1884 -1.7801 0.95549 0.18023 -0.47942 0.6672 0.06455 -0.89756 0.50649 0.45807 -0.4137 0.29551 -0.72757 0.35583 0.21081 0.69184 0.17948 -0.38435 -0.82156 0.34316 -0.2292 -0.1174 0.93835 -0.24956 -0.21212 -1.2493 0.34025 -0.60752 0.15362 -0.38889 0.27859 -1.0553 -0.10272 0.71688 0.46476 -0.24386 0.60731 -0.14626 0.49906 -0.031597 -1.3152 -0.37701 -0.15905 0.81663 -0.28852 -0.028202 0.2644 0.36998 -0.54607 -0.26863 -0.10398 -0.16196 -0.71419 0.61126 -0.81195 -0.15081 0.49439 0.31647 0.74124 -0.37209 0.68943 0.19744 0.1258 -0.22018 -0.96408 -0.46702 -0.13894 0.30874 0.50665 0.17943 0.71153 -0.00026381 -0.47585 -0.86687 -0.63838 0.3674 -0.027812 -0.50548 -0.0050586 1.1909 -0.2514 0.422 0.0074521 0.78066 0.13396 0.0063949 0.54671 0.21331 -0.063979 -0.71203 -0.34413 -0.10592 -1.664 0.059908 0.035519 0.74303 -0.71156 -0.083368 0.046354 -0.27944 -0.22258 -0.24948 0.76799 0.012705 -0.55539 0.064894 0.14826 0.41538 0.27372 -0.059421 -0.076653 0.06399 0.083734 0.20121 0.17862 -0.30495 0.2237 0.51722 -0.091633 0.17208 0.12375 0.67001 -0.24705 -0.022074 0.60573 -0.093634 0.18453 0.31808 0.088848 0.40591 0.13941 -1.114 -0.73248 1.0375 -0.36915 1.116 0.5766 0.16222 0.34327 0.46721 0.30673 -0.413 0.041562 0.090004 0.93973 0.18829 0.395 0.10011 1.1547 -0.057363 0.40281 0.77703 -0.4892 -0.73961 0.019956 0.66545 0.28972 -0.023978 0.45869 0.43236 0.64363 0.18573 -0.78063 0.050466 -0.091639 0.40486 0.12737 0.83952 1.0197 0.13825 0.6778 -0.45252 -0.43476 0.69962 0.43987 0.044541 1.1151 0.29316 0.55698 0.29285 0.51472 0.12164 -0.17263 0.68153 -0.15713 -0.0082957 0.052173 0.22946 -0.40682 -0.057423 0.34515 -0.2588 -0.71888 0.27499 0.56278 0.59652 -0.82314 0.53207 -0.56464 0.35201 -0.81956 -0.45959 -0.11181 0.002767 0.20839 -0.11993 -0.16981 0.34309 -0.53543 0.12176 0.15777 -0.22412 0.10459 -0.55587 0.36915 0.014192 -0.61869 -0.34567 -0.60151 -0.1234 -0.59543 -0.62935 -0.35452 0.29317 0.092327 -0.044436 -0.48798 0.9649 0.45625 -0.0059057 -0.43416 -0.71245 0.22928 -0.1943 -0.058306 0.016294 -0.67552 0.49185 0.72063 -0.58111 0.43908 -0.052694 -0.27258 -0.14604 -0.70854 -0.26019 -0.077697 -1.2106 0.23728 0.42626 -1.0457 0.17442 -0.62781 0.80538 0.079948 0.20073 0.2751 0.292 -0.07767 0.12779 0.37107 -0.0064965 0.037248 -0.39539 0.050184 0.20656 0.38481 -0.87068 0.56231 -0.33836 0.14319 0.25696 0.36331 -0.38799 0.58948 -0.33161 -0.071877 0.40965 0.13535 -0.030141 1.0225 0.082464 -0.95272 0.0059679\n",
            "Loaded 2195884 word vectors.\n",
            "Epoch 1/30\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 861ms/step - accuracy: 0.7234 - loss: 0.9447 - val_accuracy: 0.8386 - val_loss: 0.5360\n",
            "Epoch 2/30\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 880ms/step - accuracy: 0.8061 - loss: 0.5630 - val_accuracy: 0.8291 - val_loss: 0.5053\n",
            "Epoch 3/30\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 890ms/step - accuracy: 0.8191 - loss: 0.4770 - val_accuracy: 0.8468 - val_loss: 0.4351\n",
            "Epoch 4/30\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 900ms/step - accuracy: 0.8272 - loss: 0.4366 - val_accuracy: 0.8452 - val_loss: 0.4401\n",
            "Epoch 5/30\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 890ms/step - accuracy: 0.8333 - loss: 0.4106 - val_accuracy: 0.7872 - val_loss: 0.5461\n",
            "Epoch 6/30\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 879ms/step - accuracy: 0.8290 - loss: 0.4227 - val_accuracy: 0.7822 - val_loss: 0.5244\n",
            "Epoch 7/30\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 885ms/step - accuracy: 0.8320 - loss: 0.3805 - val_accuracy: 0.7803 - val_loss: 0.5330\n",
            "Epoch 8/30\n",
            "\u001b[1m397/397\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 827ms/step - accuracy: 0.8451 - loss: 0.3752 - val_accuracy: 0.7699 - val_loss: 0.5503\n",
            "\u001b[1m124/124\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 249ms/step - accuracy: 0.8561 - loss: 0.4155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.4326091408729553, Test accuracy: 0.848461925983429\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Predefined variables\n",
        "vocab_size = 10000  # Adjust as necessary\n",
        "embedding_dim = 300  # Your GloVe embedding dimension\n",
        "maxlen = 100  # Adjust maxlen as needed\n",
        "\n",
        "# Load GloVe embeddings function\n",
        "glove_file_path = '/content/glove.840B.300d.txt'\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            try:\n",
        "                coefs = np.asarray(values[1:], dtype='float32')\n",
        "                embeddings_index[word] = coefs\n",
        "            except ValueError:\n",
        "                print(f\"Skipping line: {line.strip()}\")  # Log the skipped line\n",
        "    return embeddings_index\n",
        "\n",
        "# Load the GloVe embeddings\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "# Check the number of loaded embeddings\n",
        "print(f\"Loaded {len(glove_embeddings)} word vectors.\")\n",
        "\n",
        "# Prepare the embedding matrix based on the loaded GloVe embeddings\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "        embedding_vector = glove_embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize the label encoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform labels for both training and testing labels\n",
        "Y_train_enc = label_encoder.fit_transform(Y_train)\n",
        "Y_test_enc = label_encoder.transform(Y_test)\n",
        "\n",
        "# Ensure Y_train_enc and Y_test_enc are numpy arrays with integer dtype\n",
        "Y_train_enc = np.array(Y_train_enc, dtype=np.int32)\n",
        "Y_test_enc = np.array(Y_test_enc, dtype=np.int32)\n",
        "\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(Y_train_enc), y=Y_train_enc)\n",
        "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Define the model\n",
        "def create_model(vocab_size, embedding_dim, input_length):\n",
        "    input_layer = Input(shape=(input_length,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
        "                                weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    bilstm_layer = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
        "    dropout_layer = Dropout(0.5)(bilstm_layer)\n",
        "    bilstm_layer2 = Bidirectional(LSTM(64))(dropout_layer)\n",
        "    dropout_layer2 = Dropout(0.5)(bilstm_layer2)\n",
        "    output_layer = Dense(3, activation='softmax')(dropout_layer2)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return model\n",
        "\n",
        "# Create and compile the model\n",
        "model = create_model(vocab_size, embedding_dim, maxlen)\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),  # Lower learning rate\n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "X_train_pad = np.array(X_train_pad, dtype=np.int32)\n",
        "Y_train_enc = np.array(Y_train_enc, dtype=np.int32)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, Y_train_enc, epochs=30, batch_size=32, validation_split=0.2,\n",
        "                    class_weight=class_weights, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_pad, Y_test_enc)\n",
        "print(f\"Test loss: {test_loss}, Test accuracy: {test_accuracy}\")\n",
        "\n",
        "model.save('/content/hate_speech_detection_model.h5')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fdec2d8a0f794774be04770baa80debf",
            "12ee96034efe424da0d8ad6992b4a696",
            "1979ac854dae4f98a2b72988215b4007",
            "c2042d5f67204d3ba80f0dc58897c0bb",
            "e54d7b766bc248f8b4340258ac1ceea6",
            "f6a52141456a42aeb83fd908c8ac6743",
            "9963cf2927924731afc10c83a0cf584a",
            "e58f1defd7244db1aaf00c0c476109a6",
            "03d67105f6314ac696e6259fcddecc28",
            "d8bad80ee8d4433a88d400f62601f67b",
            "85e700a778a046ce9bfc50f34197fa91",
            "630ce8a104334c62aa67d488e08d1f0f",
            "787bc6342c6b40cf8946433dc9266027",
            "fc6be2dcd1d242d79bae6e4d695f37da",
            "97b6d2e802bf45e39090b58e6a48050f",
            "25590986d5814beb8300f515e6c5de0b",
            "15f2ede3ff2a4fc28f1f66b2c781a424",
            "9b96d9a75e754886acbc0479b830bdc8",
            "009627b3ef72418cac06ccbb81b8ad53",
            "30635f32f3c64829be739abfab5e87bf",
            "d858639d499d4bd99b18dbf07de5eadd",
            "5c90fdc88b30433992740e37af2dac18",
            "3dccd6490ba5426db09432063313527f",
            "4ddb0618c6154ababe9ae0a03be90e1a",
            "eb6fe59ee12640f18a8c8681a0c57b1e",
            "6f480df87a7a478eaf710dc7c4e5f75b",
            "04d7372aa7f446c39f1c6579a9b3234b",
            "84fd1482e3954472816b1cbf2425d6b3",
            "e00f2b7c4fd64568ad61befe9d9bc2c1",
            "26d12c527a3a46209b366f1f2238e938",
            "a4fa6ac731084d7587ba01f2dccb3743",
            "257689a6f2d4420e9433a32dcd4bd819",
            "096245faa64343ef9622de9b510bda90"
          ]
        },
        "id": "dCZecNUqhc6B",
        "outputId": "a1aae0f6-90ed-4281-9c67-be793280005c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/5.92k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdec2d8a0f794774be04770baa80debf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/1.63M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "630ce8a104334c62aa67d488e08d1f0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/24783 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dccd6490ba5426db09432063313527f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Counts in the Dataset:\n",
            "class\n",
            "1    19190\n",
            "2     4163\n",
            "0     1430\n",
            "Name: count, dtype: int64\n",
            "\u001b[1m155/155\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 228ms/step\n",
            "Predicted Classes Distribution:\n",
            "0    3356\n",
            "2     957\n",
            "1     644\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Probabilities and Predicted Classes:\n",
            "   prob_class_0  prob_class_1  prob_class_2  predClass\n",
            "0      0.923629      0.064692      0.011679          0\n",
            "1      0.879779      0.110796      0.009425          0\n",
            "2      0.027485      0.029542      0.942973          2\n",
            "3      0.165634      0.826099      0.008267          1\n",
            "4      0.779621      0.212773      0.007606          0\n"
          ]
        }
      ],
      "source": [
        "# Required imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords for NLTK if not already installed\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/hate_speech_detection_model.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/tokenizer.pkl', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Install and import the Hugging Face `datasets` library to load data\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"tdavidson/hate_speech_offensive\")\n",
        "\n",
        "# Convert the Hugging Face Dataset to a pandas DataFrame\n",
        "data = dataset['train'].to_pandas()\n",
        "\n",
        "# Display the class distribution to understand the dataset\n",
        "class_counts = data['class'].value_counts()\n",
        "print(\"Class Counts in the Dataset:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Split the dataset into 80% training and 20% testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preprocessing function for text cleaning\n",
        "def text_processing(raw_text):\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text)  # Remove non-letters\n",
        "    words = letters_only.lower().split()  # Convert to lowercase and split\n",
        "    stops = set(stopwords.words(\"english\"))  # Define stopwords\n",
        "    meaningful_words = [w for w in words if w not in stops]  # Remove stopwords\n",
        "    return \" \".join(meaningful_words)  # Join words back into a single string\n",
        "\n",
        "# Process text in the training and testing datasets\n",
        "train_data[\"clean_text\"] = train_data[\"tweet\"].apply(text_processing)\n",
        "test_data[\"clean_text\"] = test_data[\"tweet\"].apply(text_processing)\n",
        "\n",
        "# Split the cleaned text and labels for testing\n",
        "X_test = test_data[\"clean_text\"]\n",
        "Y_test = test_data[\"class\"]\n",
        "\n",
        "# Convert test text to sequences using tokenizer and pad them to the correct length\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=100)  # Adjust `maxlen` as per model input\n",
        "\n",
        "# Get predictions from the model\n",
        "predicted = model.predict(X_test_pad)\n",
        "\n",
        "# Convert predictions to class labels by finding the index of the highest probability\n",
        "predicted_classes = np.argmax(predicted, axis=1)\n",
        "\n",
        "# Print the predicted class counts\n",
        "print(\"Predicted Classes Distribution:\")\n",
        "print(pd.Series(predicted_classes).value_counts())\n",
        "\n",
        "# Display the probabilities for each class for sample inspection (optional)\n",
        "dataframe = pd.DataFrame(predicted, columns=['prob_class_0', 'prob_class_1', 'prob_class_2'])\n",
        "dataframe['predClass'] = predicted_classes\n",
        "print(\"\\nProbabilities and Predicted Classes:\")\n",
        "print(dataframe.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "mjZhFrWk1cdE",
        "outputId": "f32d85db-0249-4ba6-f280-771cb7fa1cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Choose your sensitivity level:\n",
            "1: Very Sensitive\n",
            "2: Sensitive\n",
            "3: Neutral\n",
            "4: Less Sensitive\n",
            "5: Nothing Hurts Me No More\n",
            "Enter the number corresponding to your choice (or 'exit' to quit): 3\n",
            "Using threshold of 0.50 based on sensitivity level 'Neutral'.\n",
            "Enter text (or 'exit' to quit): women should obey husbands\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539ms/step\n",
            "Predicted class by new model: Uncertain Classification\n",
            "\n",
            "Choose your sensitivity level:\n",
            "1: Very Sensitive\n",
            "2: Sensitive\n",
            "3: Neutral\n",
            "4: Less Sensitive\n",
            "5: Nothing Hurts Me No More\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d4600284ff37>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key}: {label}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0muser_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the number corresponding to your choice (or 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_choice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model('/content/hate_speech_detection_model.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/tokenizer.pkl', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "# Function to map model output to class labels based on user-defined threshold\n",
        "def get_label_with_threshold(probabilities, threshold):\n",
        "    labels = ['Hate Speech', 'Offensive Language', 'Neither']\n",
        "\n",
        "    # Extract probabilities directly\n",
        "    hate_speech_prob = probabilities[0]\n",
        "    offensive_language_prob = probabilities[1]\n",
        "    neither_prob = probabilities[2]\n",
        "\n",
        "    # Determine label based on the thresholds\n",
        "    if (hate_speech_prob + offensive_language_prob) > threshold:\n",
        "        return \"Hate Speech\"\n",
        "    elif neither_prob > (hate_speech_prob + offensive_language_prob) + threshold:\n",
        "        return \"Not Hate Speech\"\n",
        "    else:\n",
        "        return \"Uncertain Classification\"\n",
        "\n",
        "# Preprocess and vectorize function\n",
        "def preprocess_and_vectorize(tweet, tokenizer, maxlen=100):\n",
        "    # Vectorize the tweet\n",
        "    tweet_seq = tokenizer.texts_to_sequences([tweet])\n",
        "    tweet_pad = pad_sequences(tweet_seq, maxlen=maxlen)\n",
        "    return tweet_pad\n",
        "\n",
        "# Function to determine the threshold based on sensitivity option\n",
        "def get_threshold(sensitivity_level):\n",
        "    return (10 - sensitivity_level) / 10  # Converts to a threshold between 0.0 and 0.9\n",
        "\n",
        "# Main loop for terminal interface\n",
        "if __name__ == '__main__':\n",
        "    sensitivity_options = {\n",
        "        \"1\": (\"Very Sensitive\", 9.1),\n",
        "        \"2\": (\"Sensitive\", 8.9),\n",
        "        \"3\": (\"Neutral\", 5),\n",
        "        \"4\": (\"Less Sensitive\", 1.2),\n",
        "        \"5\": (\"Nothing Hurts Me No More\", 1),\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nChoose your sensitivity level:\")\n",
        "        for key, (label, _) in sensitivity_options.items():\n",
        "            print(f\"{key}: {label}\")\n",
        "\n",
        "        user_choice = input(\"Enter the number corresponding to your choice (or 'exit' to quit): \")\n",
        "        if user_choice.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        if user_choice not in sensitivity_options:\n",
        "            print(\"Invalid choice. Please try again.\")\n",
        "            continue\n",
        "\n",
        "        sensitivity_level = sensitivity_options[user_choice][1]\n",
        "        threshold = get_threshold(sensitivity_level)\n",
        "\n",
        "        print(f\"Using threshold of {threshold:.2f} based on sensitivity level '{sensitivity_options[user_choice][0]}'.\")\n",
        "\n",
        "        user_input = input(\"Enter text (or 'exit' to quit): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Preprocess and vectorize the text using the new tokenizer\n",
        "        processed_tweet = preprocess_and_vectorize(user_input, tokenizer)\n",
        "\n",
        "        # Pass the text to the new model\n",
        "        prediction_new_model = model.predict(processed_tweet)\n",
        "\n",
        "        # Get predicted class with the threshold\n",
        "        predicted_class_new_model = get_label_with_threshold(prediction_new_model[0], threshold)\n",
        "\n",
        "        print(f\"Predicted class by new model: {predicted_class_new_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY_MOnEh_56u",
        "outputId": "67026643-9545-483f-8212-89341baa1b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install Flask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKItVu6IEel2",
        "outputId": "121ed08c-588a-4d7a-ee48-6c8d4a8c92f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, render_template, request, jsonify\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load your model and tokenizer here\n",
        "model = load_model('/content/hate_speech_detection_model.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/tokenizer.pkl', 'rb') as f:  # Replace with your tokenizer filename\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Function to map model output to class labels based on user-defined threshold\n",
        "def get_label_with_threshold(probabilities, threshold):\n",
        "    hate_speech_prob = probabilities[0]\n",
        "    offensive_language_prob = probabilities[1]\n",
        "    neither_prob = probabilities[2]\n",
        "\n",
        "    if (hate_speech_prob + offensive_language_prob) > threshold:\n",
        "        return \"Hate Speech\"\n",
        "    elif neither_prob > (hate_speech_prob + offensive_language_prob) + threshold:\n",
        "        return \"Not Hate Speech\"\n",
        "    else:\n",
        "        return \"Uncertain Classification\"\n",
        "\n",
        "def preprocess_and_vectorize(text, tokenizer, maxlen=100):\n",
        "    tweet_seq = tokenizer.texts_to_sequences([text])\n",
        "    tweet_pad = pad_sequences(tweet_seq, maxlen=maxlen)\n",
        "    return tweet_pad\n",
        "\n",
        "def get_threshold(sensitivity_level):\n",
        "    return (10 - sensitivity_level) / 10\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return '''\n",
        "        <h1>Hate Speech Detection</h1>\n",
        "        <form action=\"/predict\" method=\"post\">\n",
        "            <label for=\"sensitivity\">Choose your sensitivity level:</label>\n",
        "            <input type=\"range\" id=\"sensitivity\" name=\"sensitivity\" min=\"1\" max=\"10\" step=\"0.5\" value=\"5\">\n",
        "            <br>\n",
        "            <label for=\"text\">Enter text:</label>\n",
        "            <input type=\"text\" id=\"text\" name=\"text\" placeholder=\"Type your text here\">\n",
        "            <br>\n",
        "            <input type=\"submit\" value=\"Submit\">\n",
        "        </form>\n",
        "        <div id=\"result\"></div>\n",
        "    '''\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    text = request.form['text']\n",
        "    sensitivity_level = float(request.form['sensitivity'])\n",
        "    threshold = get_threshold(sensitivity_level)\n",
        "\n",
        "    # Preprocess and vectorize the text\n",
        "    processed_text = preprocess_and_vectorize(text, tokenizer)\n",
        "    prediction = model.predict(processed_text)\n",
        "\n",
        "    # Get predicted class with the threshold\n",
        "    predicted_class = get_label_with_threshold(prediction[0], threshold)\n",
        "\n",
        "    return f'Predicted class: {predicted_class}'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbQpBjHBDCC0",
        "outputId": "fe166798-3834-45f3-98a7-8782564a0e1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.0.6)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install flask pyngrok keras numpy pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPw8kz3XHZ0z",
        "outputId": "4cae4264-5e9f-4722-ff93-3425b1b4136c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "96C84EzPylS3",
        "outputId": "9569ee89-10fc-4007-a792-2b5af466b140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Ngrok tunnel \"NgrokTunnel: \"https://3530-34-82-3-157.ngrok-free.app\" -> \"http://localhost:5001\"\" -> \"http://127.0.0.1:5001\"\n",
            "2024-11-11 16:45:43.720425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-11 16:45:43.843127: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-11 16:45:43.863105: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-11 16:45:48.961261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-11-11T16:45:50+0000 lvl=warn msg=\"failed to open private leg\" id=ac9fd1674165 privaddr=localhost:5001 err=\"dial tcp 127.0.0.1:5001: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-11-11T16:45:51+0000 lvl=warn msg=\"failed to open private leg\" id=0301f7fa9334 privaddr=localhost:5001 err=\"dial tcp 127.0.0.1:5001: connect: connection refused\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            " * Serving Flask app 'app'\n",
            " * Debug mode: off\n",
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e50403cede0d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Run your Flask app in the background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python app.py &'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    202\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok authtoken\n",
        "ngrok.set_auth_token('2nL2KHLz1GLeSC9jIw3bkNExDh3_7rnirNnmMAnuAFqaBG8Az')\n",
        "\n",
        "# Start the ngrok tunnel on port 5001\n",
        "public_url = ngrok.connect(5001)\n",
        "print(\" * Ngrok tunnel \\\"{}\\\" -> \\\"http://127.0.0.1:5001\\\"\".format(public_url))\n",
        "\n",
        "# Run your Flask app in the background\n",
        "!python app.py &\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl73GnjSf+2ZlY3cE171C9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}